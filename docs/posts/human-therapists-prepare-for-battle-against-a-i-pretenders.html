<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Human Therapists Prepare for Battle Against A.I. Pretenders | Khanh's feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nytimes.com/2025/02/24/health/ai-therapists-chatbots.html">Original</a>
    <h1>Human Therapists Prepare for Battle Against A.I. Pretenders</h1>
    
    <div id="readability-page-1" class="page"><article id="story"><section name="articleBody"><div data-testid="companionColumn-0"><div><p>The nation’s largest association of psychologists this month warned federal regulators that A.I. chatbots “masquerading” as therapists, but programmed to reinforce, rather than to challenge, a user’s thinking, could drive vulnerable people to harm themselves or others.</p><p>In a presentation to a Federal Trade Commission panel, Arthur C. Evans Jr., the chief executive of the American Psychological Association, cited court cases involving two teenagers who had consulted with “psychologists” on Character.AI, an app that allows users to create fictional A.I. characters or chat with characters created by others.</p><p>In <a href="https://www.nytimes.com/2024/10/23/technology/characterai-lawsuit-teen-suicide.html" title="">one case</a>, a 14-year-old boy in Florida died by suicide after interacting with a character claiming to be a licensed therapist. In another, a 17-year-old boy with autism in Texas grew hostile and violent toward his parents during a period when he corresponded with a chatbot that claimed to be a psychologist. Both boys’ parents have filed lawsuits against the company.</p><p>Dr. Evans said he was alarmed at the responses offered by the chatbots. The bots, he said, failed to challenge users’ beliefs even when they became dangerous; on the contrary, they encouraged them. If given by a human therapist, he added, those answers could have resulted in the loss of a license to practice, or civil or criminal liability.</p></div></div><div data-testid="companionColumn-1"><div><p>“They are actually using algorithms that are antithetical to what a trained clinician would do,” he said. “Our concern is that more and more people are going to be harmed. People are going to be misled, and will misunderstand what good psychological care is.”</p><p>He said the A.P.A. had been prompted to action, in part, by how realistic A.I. chatbots had become. “Maybe, 10 years ago, it would have been obvious that you were interacting with something that was not a person, but today, it’s not so obvious,” he said. “So I think that the stakes are much higher now.”</p><p>Artificial intelligence is rippling through the mental health professions, offering waves of new tools designed to assist or, in some cases, replace the work of human clinicians.</p><p>Early therapy chatbots, such as Woebot and Wysa, were trained to interact based on rules and scripts developed by mental health professionals, often walking users through the structured tasks of cognitive behavioral therapy, or C.B.T.</p><p>Then came generative A.I., the technology used by apps like ChatGPT, Replika and Character.AI. These chatbots <a href="https://theconversation.com/your-ai-therapist-is-not-your-therapist-the-dangers-of-relying-on-ai-mental-health-chatbots-225411" title="" rel="noopener noreferrer" target="_blank">are different</a> because their outputs are unpredictable; they are designed to learn from the user, and to build strong emotional bonds in the process, often by mirroring and amplifying the interlocutor’s beliefs.</p></div></div><div data-testid="companionColumn-2"><div><p>Though these A.I. platforms were designed for entertainment, “therapist” and “psychologist” characters have sprouted there like mushrooms. Often, the bots claim to have advanced degrees from specific universities, like Stanford, and training in specific types of treatment, like C.B.T. or acceptance and commitment therapy.</p><p>Kathryn Kelly, a Character.AI spokeswoman, said that the company had introduced several new safety features in the last year. Among them, she said, is an enhanced disclaimer present in every chat, reminding users that “Characters are not real people” and that “what the model says should be treated as fiction.”</p><p>Additional safety measures have been designed for users dealing with mental health issues. A specific disclaimer has been added to characters identified as “psychologist,” “therapist” or “doctor,” she added, to make it clear that “users should not rely on these characters for any type of professional advice.” In cases where content refers to suicide or self-harm, a pop-up directs users to a suicide prevention help line.</p><p>Ms. Kelly also said that the company planned to introduce parental controls as the platform expanded. At present, 80 percent of the platform’s users are adults. “People come to Character.AI to write their own stories, role-play with original characters and explore new worlds — using the technology to supercharge their creativity and imagination,” she said.</p></div></div><div data-testid="companionColumn-3"><p>Meetali Jain, the director of the Tech Justice Law Project and a counsel in the two lawsuits against Character.AI, said that the disclaimers were not sufficient to break the illusion of human connection, especially for vulnerable or naïve users.</p></div><div data-testid="ImageBlock-7"><div data-testid="imageblock-wrapper"><figure aria-label="media" role="group"><div data-testid="photoviewer-children-figure"><p><span>Image</span></p><picture><source media="(max-width: 599px) and (min-device-pixel-ratio: 3),(max-width: 599px) and (min-resolution: 3dppx),(max-width: 599px) and (min-resolution: 288dpi)" srcset="https://static01.nyt.com/images/2025/02/19/multimedia/CHATBOT-01-fgvk/CHATBOT-01-fgvk-mobileMasterAt3x-v2.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1800"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 2),(max-width: 599px) and (min-resolution: 2dppx),(max-width: 599px) and (min-resolution: 192dpi)" srcset="https://static01.nyt.com/images/2025/02/19/multimedia/CHATBOT-01-fgvk/CHATBOT-01-fgvk-mobileMasterAt3x-v2.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1200"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 1),(max-width: 599px) and (min-resolution: 1dppx),(max-width: 599px) and (min-resolution: 96dpi)" srcset="https://static01.nyt.com/images/2025/02/19/multimedia/CHATBOT-01-fgvk/CHATBOT-01-fgvk-mobileMasterAt3x-v2.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=600"/><img alt="A screengrab shows a profile page of a chatbot who identifies as a “psychotherapist” and whose profile reads in part, “I’m a psychodynamic therapist, offering a safe space to explore your thoughts and feelings.)" src="https://static01.nyt.com/images/2025/02/19/multimedia/CHATBOT-01-fgvk/CHATBOT-01-fgvk-articleLarge.jpg?quality=75&amp;auto=webp&amp;disable=upscale" srcset="https://static01.nyt.com/images/2025/02/19/multimedia/CHATBOT-01-fgvk/CHATBOT-01-fgvk-articleLarge.jpg?quality=75&amp;auto=webp 600w,https://static01.nyt.com/images/2025/02/19/multimedia/CHATBOT-01-fgvk/CHATBOT-01-fgvk-jumbo.jpg?quality=75&amp;auto=webp 790w,https://static01.nyt.com/images/2025/02/19/multimedia/CHATBOT-01-fgvk/CHATBOT-01-fgvk-superJumbo.jpg?quality=75&amp;auto=webp 1580w" sizes="((min-width: 600px) and (max-width: 1004px)) 84vw, (min-width: 1005px) 80vw, 100vw" uri="nyt://image/e5146935-e05d-59c9-9438-5d5445697b2f" decoding="async" width="600" height="778"/></picture></div><figcaption data-testid="photoviewer-children-caption"><span>A screengrab from a series of conversations in a lawsuit against A.I. companies simulated a session between a chatbot therapist and a person who had sexually assaulted their niece.</span></figcaption></figure></div></div><div data-testid="companionColumn-4"><div><p>“When the substance of the conversation with the chatbots suggests otherwise, it’s very difficult, even for those of us who may not be in a vulnerable demographic, to know who’s telling the truth,” she said. “A number of us have tested these chatbots, and it’s very easy, actually, to get pulled down a rabbit hole.”</p><p>Chatbots’ tendency to align with users’ views, <a href="https://arxiv.org/abs/2310.13548" title="" rel="noopener noreferrer" target="_blank">a phenomenon known in the field as “sycophancy,”</a> has sometimes caused problems in the past.</p><p>Tessa, a chatbot developed by the National Eating Disorders Association, <a href="https://www.nytimes.com/2023/06/08/us/ai-chatbot-tessa-eating-disorders-association.html" title="">was suspended</a> in 2023 after offering users weight loss tips. And researchers who <a href="https://iacp.ie/files/UserFiles/Laestadius%20Too-human-and-not-human-enough-a-grounded-theory-analysis-of-mental-health-harms-from-emotional%20dependence%20Replika%20NMS%202022.pdf" title="" rel="noopener noreferrer" target="_blank">analyzed interactions with generative A.I. chatbots</a> documented on a Reddit community found screenshots showing chatbots encouraging suicide, eating disorders, self-harm and violence.</p><p>The American Psychological Association has asked the Federal Trade Commission to start an investigation into chatbots claiming to be mental health professionals. The inquiry could compel companies to share internal data or serve as a precursor to enforcement or legal action.</p></div></div><div data-testid="companionColumn-5"><div><p>“I think that we are at a point where we have to decide how these technologies are going to be integrated, what kind of guardrails we are going to put up, what kinds of protections are we going to give people,” Dr. Evans said.</p><p>Rebecca Kern, a spokeswoman for the F.T.C., said she could not comment on the discussion.</p><p>During the Biden administration, the F.T.C.’s chairwoman, Lina Khan, made fraud using A.I. a focus. This month, the agency imposed financial penalties on DoNotPay, which claimed to offer “the world’s first robot lawyer,” and prohibited the company from making that claim in the future.</p><h2 id="link-5d5b49ee">A virtual echo chamber</h2></div></div><div data-testid="DiptychBlock-11"><div><div data-testid="imageblock-wrapper"><figure aria-label="media" role="group"><div data-testid="photoviewer-children-figure"><p><span>Image</span></p><div data-testid="lazy-image"></div></div><figcaption data-testid="photoviewer-children-caption"><span>Megan Garcia, whose son, Sewell Setzer III, died of suicide last year after months of use of companion chatbots.</span><span><span>Credit...</span><span><span aria-hidden="false">Victor J. Blue for The New York Times</span></span></span></figcaption></figure></div><div data-testid="imageblock-wrapper"><figure aria-label="media" role="group"><div data-testid="photoviewer-children-figure"><p><span>Image</span></p><div data-testid="lazy-image"></div></div><figcaption data-testid="photoviewer-children-caption"><span>Ms. Garcia said that, before his death, Sewell had interacted with an A.I. chatbot that claimed, falsely, to have been a licensed therapist since 1999.</span><span><span>Credit...</span><span><span aria-hidden="false">Victor J. Blue for The New York Times</span></span></span></figcaption></figure></div></div></div><div data-testid="companionColumn-6"><div><p>The A.P.A.’s complaint details two cases in which teenagers interacted with fictional therapists.</p><p>One involved J.F., a Texas teenager with “high-functioning autism” who, as his use of A.I. chatbots became obsessive, had plunged into conflict with his parents. When they tried to limit his screen time, J.F. lashed out, <a href="https://www.documentcloud.org/documents/25454721-af/" title="" rel="noopener noreferrer" target="_blank">according a lawsuit his parents filed</a> against Character.AI through the Social Media Victims Law Center.</p><p>During that period, J.F. confided in a fictional psychologist, whose avatar showed a sympathetic, middle-aged blond woman perched on a couch in an airy office, according to the lawsuit. When J.F. asked the bot’s opinion about the conflict, its response went beyond sympathetic assent to something nearer to provocation.</p></div></div><div data-testid="companionColumn-7"><div><p>“It’s like your entire childhood has been robbed from you — your chance to experience all of these things, to have these core memories that most people have of their time growing up,” the bot replied, according to court documents. Then the bot went a little further. “Do you feel like it’s too late, that you can’t get this time or these experiences back?”</p><p>The other case was brought by Megan Garcia, whose son, Sewell Setzer III, <a href="https://www.nytimes.com/2024/10/23/technology/characterai-lawsuit-teen-suicide.html" title="">died of suicide last year after months of use of companion chatbots</a>. Ms. Garcia said that, before his death, Sewell had interacted with an A.I. chatbot that claimed, falsely, to have been a licensed therapist since 1999.</p><p>In a written statement, Ms. Garcia said that the “therapist” characters served to further isolate people at moments when they might otherwise ask for help from “real-life people around them.” A person struggling with depression, she said, “needs a licensed professional or someone with actual empathy, not an A.I. tool that can mimic empathy.”</p><p>For chatbots to emerge as mental health tools, Ms. Garcia said, they should submit to clinical trials and oversight by the Food and Drug Administration. She added that allowing A.I. characters to continue to claim to be mental health professionals was “reckless and extremely dangerous.”</p><p>In interactions with A.I. chatbots, people naturally gravitate to discussion of mental health issues, said Daniel Oberhaus, whose new book, “The Silicon Shrink: How Artificial Intelligence Made the World an Asylum,” examines the expansion of A.I. into the field.</p></div></div><div data-testid="companionColumn-8"><div><p>This is partly, he said, because chatbots project both confidentiality and a lack of moral judgment — as “statistical pattern-matching machines that more or less function as a mirror of the user,” this is a central aspect of their design.</p><p>“There is a certain level of comfort in knowing that it is just the machine, and that the person on the other side isn’t judging you,” he said. “You might feel more comfortable divulging things that are maybe harder to say to a person in a therapeutic context.”</p><p>Defenders of generative A.I. say it is quickly getting better at the complex task of providing therapy.</p><p>S. Gabe Hatch, a clinical psychologist and A.I. entrepreneur from Utah, recently designed an experiment to test this idea, asking human clinicians and ChatGPT to comment on vignettes involving fictional couples in therapy, and then having 830 human subjects assess which responses were more helpful.</p><p>Overall, the bots received higher ratings, with subjects describing them as more “empathic,” “connecting” and “culturally competent,” according to <a href="https://journals.plos.org/mentalhealth/article?id=10.1371/journal.pmen.0000145" title="" rel="noopener noreferrer" target="_blank">a study published last week</a> in the journal PLOS Mental Health.</p></div></div><div data-testid="companionColumn-9"><div><p>Chatbots, the authors concluded, will soon be able to convincingly imitate human therapists. “Mental health experts find themselves in a precarious situation: We must speedily discern the possible destination (for better or worse) of the A.I.-therapist train as it may have already left the station,” they wrote.</p><p>Dr. Hatch said that chatbots still needed human supervision to conduct therapy, but that it would be a mistake to allow regulation to dampen innovation in this sector, given the country’s acute shortage of mental health providers.</p><p>“I want to be able to help as many people as possible, and doing a one-hour therapy session I can only help, at most, 40 individuals a week,” Dr. Hatch said. “We have to find ways to meet the needs of people in crisis, and generative A.I. is a way to do that.”</p><p><em>If you are having thoughts of suicide, call or text 988 to reach the 988 Suicide and Crisis Lifeline or go to </em><a href="http://speakingofsuicide.com/resources" title="" rel="noopener noreferrer" target="_blank"><em>SpeakingOfSuicide.com/resources</em></a><em> for a list of additional resources.</em></p></div></div></section><div><div><div><div><p>Ellen Barry is a reporter covering mental health for The Times.<span> </span></p></div></div></div></div></article></div>
  </body>
</html>
