<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>If A.I. Systems Become Conscious, Should They Have Rights? | Khanh's feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nytimes.com/2025/04/24/technology/ai-welfare-anthropic-claude.html">Original</a>
    <h1>If A.I. Systems Become Conscious, Should They Have Rights?</h1>
    
    <div id="readability-page-1" class="page"><div id="site-content"><div><article id="story"><div><div><div><div><div><nav aria-labelledby="storyline-menu-title" role="navigation"><p id="storyline-menu-title"><a href="https://www.nytimes.com/spotlight/artificial-intelligence"><span><span data-testid="text-balancer">Artificial</span><span data-testid="text-balancer"> Intelligence</span></span></a></p><ul role="menu"><li><span data-testid="menu-link"><a role="menuitem" href="https://www.nytimes.com/2025/04/03/technology/ai-futures-project-ai-2027.html"><span>A.I. Forecast</span></a></span></li><li><span data-testid="menu-link"><a role="menuitem" href="https://www.nytimes.com/2025/03/25/technology/nvidia-ai-robots.html"><span>A.I.’s Super Bowl</span></a></span></li><li><span data-testid="menu-link"><a role="menuitem" href="https://www.nytimes.com/2025/03/11/technology/google-investment-anthropic.html"><span>Google’s Anthropic Investment</span></a></span></li><li><span data-testid="menu-link"><a role="menuitem" href="https://www.nytimes.com/2025/02/27/technology/personaltech/vibecoding-ai-software-programming.html"><span>What Is Vibecoding?</span></a></span></li><li><span data-testid="menu-link"><a role="menuitem" href="https://www.nytimes.com/interactive/2024/12/27/technology/artificial-intelligence-generative-fill-photoshop-openai.html"><span>Quiz</span></a></span></li></ul></nav></div></div></div></div></div><div id="top-wrapper"><p>Advertisement</p><p><a href="#after-top">SKIP ADVERTISEMENT</a></p><div></div></div><header><div id="sponsor-wrapper"><p>Supported by</p><p><a href="#after-sponsor">SKIP ADVERTISEMENT</a></p></div><p>the shift</p><p id="article-summary">As artificial intelligence systems become smarter, one A.I. company is trying to figure out what to do if they become conscious.</p><div><div><div><div><div><div><p>Listen to this article </p><!-- --><p>· 7:43 min</p><!-- --> <p><span><a href="https://help.nytimes.com/hc/en-us/articles/24318293692180">Learn more</a></span></p></div></div></div></div></div><div><div role="toolbar" data-testid="share-tools" aria-label="Social Media Share buttons, Save button, and Comments Panel with current comment count"><div data-testid="share-tools-menu"><ul data-testid="share-tools-list"><li><div></div></li><li><div></div></li><li></li><li><span><div><svg aria-hidden="true" width="21" height="18" viewBox="0 0 21 18"><path d="m14.52 17.831-5.715-4.545H2.4a1.468 1.468 0 0 1-1.468-1.469V1.894A1.471 1.471 0 0 1 2.4.405h16.583a1.469 1.469 0 0 1 1.469 1.469v9.923a1.469 1.469 0 0 1-1.47 1.47H14.58l-.06 4.564ZM2.4 1.645a.228.228 0 0 0-.228.229v9.923a.228.228 0 0 0 .228.229h6.811l4.06 3.235v-3.235h5.652a.228.228 0 0 0 .229-.229V1.874a.228.228 0 0 0-.229-.229H2.4Z" fill="#121212" fill-rule="nonzero"></path></svg><p><span><a href="https://www.hltv.org/2025/04/24/technology/ai-welfare-anthropic-claude.html">170</a></span></p></div></span></li></ul></div></div></div></div><div data-testid="imageblock-wrapper"><figure aria-label="media" role="group"><div data-testid="imageContainer-children-Image"><picture><source media="(max-width: 599px) and (min-device-pixel-ratio: 3),(max-width: 599px) and (min-resolution: 3dppx),(max-width: 599px) and (min-resolution: 288dpi)" srcset="https://static01.nyt.com/images/2025/04/22/multimedia/ROOSE-1-ghvw/ROOSE-1-ghvw-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1800"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 2),(max-width: 599px) and (min-resolution: 2dppx),(max-width: 599px) and (min-resolution: 192dpi)" srcset="https://static01.nyt.com/images/2025/04/22/multimedia/ROOSE-1-ghvw/ROOSE-1-ghvw-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1200"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 1),(max-width: 599px) and (min-resolution: 1dppx),(max-width: 599px) and (min-resolution: 96dpi)" srcset="https://static01.nyt.com/images/2025/04/22/multimedia/ROOSE-1-ghvw/ROOSE-1-ghvw-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=600"/><img alt="With his hands in his pants pockets, Kyle Fish stands in a garden in front of large glass buildings." src="https://static01.nyt.com/images/2025/04/22/multimedia/ROOSE-1-ghvw/ROOSE-1-ghvw-articleLarge.jpg?quality=75&amp;auto=webp&amp;disable=upscale" srcset="https://static01.nyt.com/images/2025/04/22/multimedia/ROOSE-1-ghvw/ROOSE-1-ghvw-articleLarge.jpg?quality=75&amp;auto=webp 600w,https://static01.nyt.com/images/2025/04/22/multimedia/ROOSE-1-ghvw/ROOSE-1-ghvw-jumbo.jpg?quality=75&amp;auto=webp 1024w,https://static01.nyt.com/images/2025/04/22/multimedia/ROOSE-1-ghvw/ROOSE-1-ghvw-superJumbo.jpg?quality=75&amp;auto=webp 2048w" sizes="((min-width: 600px) and (max-width: 1004px)) 84vw, (min-width: 1005px) 80vw, 100vw" decoding="async" width="600" height="400"/></picture></div><figcaption data-testid="photoviewer-children-ImageCaption"><span>Last year, Anthropic, the company that made the Claude chatbot, hired its first A.I. welfare researcher, Kyle Fish, to study whether the company’s models were being treated humanely.</span><span><span>Credit...</span><span><span aria-hidden="false">Mike Kai Chen for The New York Times</span></span></span></figcaption></figure></div><div></div><p><time datetime="2025-04-24T12:57:41-04:00"><span>April 24, 2025</span><span>Updated <span>12:57 p.m. ET</span></span></time></p></header><section name="articleBody"><div data-testid="companionColumn-0"><div><p>One of my most deeply held values as a tech columnist is humanism. I believe in humans, and I think that technology should help people, rather than disempower or replace them. I care about aligning artificial intelligence — that is, making sure that A.I. systems act in accordance with human values — because I think our values are fundamentally good, or at least better than the values a robot could come up with.</p><p>So when I heard that researchers at Anthropic, the A.I. company that made the Claude chatbot, were starting to study “model welfare” — the idea that A.I. models might soon become conscious and deserve some kind of moral status — the humanist in me thought: <em>Who cares about the chatbots? Aren’t we supposed to be worried about A.I. mistreating us, not us mistreating it?</em></p><p>It’s hard to argue that today’s A.I. systems are conscious. Sure, large language models have been trained to talk like humans, and some of them are extremely impressive. But can ChatGPT experience joy or suffering? Does Gemini deserve human rights? Many A.I. experts I know would say no, not yet, not even close.</p><p>But I was intrigued. After all, more people are beginning to treat A.I. systems as if they are conscious — <a href="https://www.nytimes.com/2025/01/15/technology/ai-chatgpt-boyfriend-companion.html" title="">falling in love</a> with them, using them as <a href="https://www.nytimes.com/2025/04/15/health/ai-therapist-mental-health.html" title="">therapists</a> and soliciting their advice. The smartest A.I. systems are surpassing humans in some domains. Is there any threshold at which an A.I. would start to deserve, if not human-level rights, at least the same moral consideration we give to animals?</p></div></div><div data-testid="companionColumn-1"><div><p>Consciousness has long been a taboo subject within the world of serious A.I. research, where people are wary of anthropomorphizing A.I. systems for fear of seeming like cranks. (Everyone remembers what happened to Blake Lemoine, a former Google employee who was <a href="https://www.nytimes.com/2022/07/23/technology/google-engineer-artificial-intelligence.html" title="">fired in 2022</a>, after claiming that the company’s LaMDA chatbot had become sentient.)</p><p>But that may be starting to change. There is a small body of <a href="https://arxiv.org/abs/2411.00986" title="" rel="noopener noreferrer" target="_blank">academic research</a> on A.I. model welfare, and a modest but <a href="https://eleosai.org/post/experts-who-say-that-ai-welfare-is-a-serious-near-term-possibility/" title="" rel="noopener noreferrer" target="_blank">growing number</a> of experts in fields like philosophy and neuroscience are taking the prospect of A.I. consciousness more seriously, as A.I. systems grow more intelligent. Recently, the tech podcaster Dwarkesh Patel compared A.I. welfare to animal welfare, <a href="https://www.dwarkesh.com/p/ege-tamay" title="" rel="noopener noreferrer" target="_blank">saying</a> he believed it was important to make sure “the digital equivalent of factory farming” doesn’t happen to future A.I. beings.</p><p>Tech companies are starting to talk about it more, too. Google recently <a href="https://www.404media.co/google-deepmind-is-hiring-a-post-agi-research-scientist/" title="" rel="noopener noreferrer" target="_blank">posted a job listing</a> for a “post-A.G.I.” research scientist whose areas of focus will include “machine consciousness.” And last year, Anthropic <a href="https://www.transformernews.ai/p/anthropic-ai-welfare-researcher" title="" rel="noopener noreferrer" target="_blank">hired its first A.I. welfare researcher</a>, Kyle Fish.</p><p>I interviewed Mr. Fish at Anthropic’s San Francisco office last week. He’s a friendly vegan who, like a number of Anthropic employees, has ties to effective altruism, an intellectual movement with roots in the Bay Area tech scene that is focused on A.I. safety, animal welfare and other ethical issues.</p><p>Mr. Fish told me that his work at Anthropic focused on two basic questions: First, is it possible that Claude or other A.I. systems will become conscious in the near future? And second, if that happens, what should Anthropic do about it?</p></div></div><div data-testid="companionColumn-2"><p>He emphasized that this research was still early and exploratory. He thinks there’s only a small chance (maybe 15 percent or so) that Claude or another current A.I. system is conscious. But he believes that in the next few years, as A.I. models develop more humanlike abilities, A.I. companies will need to take the possibility of consciousness more seriously.</p></div><div data-testid="ImageBlock-5"><div data-testid="imageblock-wrapper"><figure aria-label="media" role="group"><div data-testid="photoviewer-children-figure"><p><span>Image</span></p><div data-testid="lazy-image"></div></div><figcaption data-testid="photoviewer-children-caption"><span>Anthropic researchers are studying how A.I. systems should be ethically treated.</span><span><span>Credit...</span><span><span aria-hidden="false">Kelsey McClellan for The New York Times</span></span></span></figcaption></figure></div></div><div data-testid="companionColumn-3"><div><p>“It seems to me that if you find yourself in the situation of bringing some new class of being into existence that is able to communicate and relate and reason and problem-solve and plan in ways that we previously associated solely with conscious beings, then it seems quite prudent to at least be asking questions about whether that system might have its own kinds of experiences,” he said.</p><p>Mr. Fish isn’t the only person at Anthropic thinking about A.I. welfare. There’s an active channel on the company’s Slack messaging system called #model-welfare, where employees check in on Claude’s well-being and share examples of A.I. systems acting in humanlike ways.</p><p>Jared Kaplan, Anthropic’s chief science officer, told me in a separate interview that he thought it was “pretty reasonable” to study A.I. welfare, given how intelligent the models are getting.</p></div></div><div data-testid="companionColumn-4"><div><p>But testing A.I. systems for consciousness is hard, Mr. Kaplan warned, because they’re such good mimics. If you prompt Claude or ChatGPT to talk about its feelings, it might give you a compelling response. That doesn’t mean the chatbot actually <em>has</em> feelings — only that it knows how to talk about them.</p><p>“Everyone is very aware that we can train the models to say whatever we want,” Mr. Kaplan said. “We can reward them for saying that they have no feelings at all. We can reward them for saying really interesting philosophical speculations about their feelings.”</p><p>So how are researchers supposed to know if A.I. systems are actually conscious or not?</p><p>Mr. Fish said it might involve using techniques borrowed from mechanistic interpretability, an A.I. subfield that studies the inner workings of A.I. systems, to check whether some of the same structures and pathways associated with consciousness in human brains are also active in A.I. systems.</p><p>You could also probe an A.I. system, he said, by observing its behavior, watching how it chooses to operate in certain environments or accomplish certain tasks, which things it seems to prefer and avoid.</p><p>Mr. Fish acknowledged that there probably wasn’t a single litmus test for A.I. consciousness. (He thinks consciousness is probably more of a spectrum than a simple yes/no switch, anyway.) But he said there were things that A.I. companies could do to take their models’ welfare into account, in case they do become conscious someday.</p></div></div><div data-testid="companionColumn-5"><div><p>One question Anthropic is exploring, he said, is whether future A.I. models should be given the ability to stop chatting with an annoying or abusive user, if they find the user’s requests too distressing.</p><p>“If a user is persistently requesting harmful content despite the model’s refusals and attempts at redirection, could we allow the model simply to end that interaction?” Mr. Fish said.</p><p>Critics might dismiss measures like these as crazy talk — today’s A.I. systems aren’t conscious by most standards, so why speculate about what they might find obnoxious? Or they might object to an A.I. company’s studying consciousness in the first place, because it might create incentives to train their systems to act more sentient than they actually are.</p><p>Personally, I think it’s fine for researchers to study A.I. welfare, or examine A.I. systems for signs of consciousness, as long as it’s not diverting resources from A.I. safety and alignment work that is aimed at keeping humans safe. And I think it’s probably a good idea to be nice to A.I. systems, if only as a hedge. (I try to say “please” and “thank you” to chatbots, even though I don’t think they’re conscious, because, as OpenAI’s <a href="https://x.com/sama/status/1912646035979239430" title="" rel="noopener noreferrer" target="_blank">Sam Altman says</a>, you never know.)</p><p>But for now, I’ll reserve my deepest concern for carbon-based life-forms. In the coming A.I. storm, it’s our welfare I’m most worried about.</p></div></div></section><div><div><div><p>Kevin Roose is a Times technology columnist and a host of the podcast &#34;<a href="https://www.nytimes.com/column/hard-fork">Hard Fork</a>.&#34;</p></div></div><div><p>See more on: <a href="https://www.nytimes.com/topic/anthropic-ai-llc">Anthropic AI LLC</a></p></div><div role="toolbar" data-testid="share-tools" aria-label="Social Media Share buttons, Save button, and Comments Panel with current comment count"><p><span><div><svg aria-hidden="true" width="21" height="18" viewBox="0 0 21 18"><path d="m14.52 17.831-5.715-4.545H2.4a1.468 1.468 0 0 1-1.468-1.469V1.894A1.471 1.471 0 0 1 2.4.405h16.583a1.469 1.469 0 0 1 1.469 1.469v9.923a1.469 1.469 0 0 1-1.47 1.47H14.58l-.06 4.564ZM2.4 1.645a.228.228 0 0 0-.228.229v9.923a.228.228 0 0 0 .228.229h6.811l4.06 3.235v-3.235h5.652a.228.228 0 0 0 .229-.229V1.874a.228.228 0 0 0-.229-.229H2.4Z" fill="#121212" fill-rule="nonzero"></path></svg><p><span><a href="https://www.hltv.org/2025/04/24/technology/ai-welfare-anthropic-claude.html">170</a></span></p></div></span></p><div data-testid="share-tools-menu"><ul data-testid="share-tools-list"><li><div></div></li><li><div></div></li><li></li><li><span><div><svg aria-hidden="true" width="21" height="18" viewBox="0 0 21 18"><path d="m14.52 17.831-5.715-4.545H2.4a1.468 1.468 0 0 1-1.468-1.469V1.894A1.471 1.471 0 0 1 2.4.405h16.583a1.469 1.469 0 0 1 1.469 1.469v9.923a1.469 1.469 0 0 1-1.47 1.47H14.58l-.06 4.564ZM2.4 1.645a.228.228 0 0 0-.228.229v9.923a.228.228 0 0 0 .228.229h6.811l4.06 3.235v-3.235h5.652a.228.228 0 0 0 .229-.229V1.874a.228.228 0 0 0-.229-.229H2.4Z" fill="#121212" fill-rule="nonzero"></path></svg><p><span><a href="https://www.hltv.org/2025/04/24/technology/ai-welfare-anthropic-claude.html">170</a></span></p></div></span></li></ul></div></div></div><div><div><section id="styln-guide" role="complementary" aria-labelledby="styln-guide-title"><hr/><h2 id="styln-guide-title">Explore Our Coverage of Artificial Intelligence</h2><hr/><p><strong>News</strong><strong> and Analysis</strong></p><ul><li><p><strong>ChatGPT:</strong> OpenAI beefed up its chatbot with new technology <a href="https://www.nytimes.com/2025/03/25/technology/chatgpt-image-generator.html">designed to generate images</a> from detailed, complex and unusual instructions.</p></li><li><p><strong>CoreWeave: </strong>The company, which provides computing power for A.I., was founded by three Bitcoin enthusiasts. The company is now set to make the <a href="https://www.nytimes.com/2025/03/18/technology/coreweave-wall-street-ai-ipo.html">first prominent A.I. initial public offering</a>.</p></li><li><p><strong>DeepSeek: </strong>Since the founder of the Chinese artificial intelligence start-up shook hands with Xi Jinping, China’s leader, officials in China have been racing to <a href="https://www.nytimes.com/2025/03/18/business/china-government-deepseek.html">show how they are using the company’s technology</a>.</p></li><li><p><strong>Turing Award: </strong>The award, often called the Nobel Prize of computing, was given to Andrew Barto and Richard Sutton, the developers of a technique called reinforcement learning that is <a href="https://www.nytimes.com/2025/03/05/technology/turing-award-andrew-barto-richard-sutton.html">vital to chatbots</a>.</p></li></ul><hr/><p><strong>The Age of A.I.</strong></p><ul><li><p><strong>Chat Therapy: </strong>Therabot is <a href="https://www.nytimes.com/2025/04/15/health/ai-therapist-mental-health.html">an A.I, chatbot researchers believe could help address an intractable problem</a>: There are too many people who need therapy and not nearly enough providers.</p></li><li><p><strong>Exploring Mysteries of Delacroix:</strong> Eric and Wendy Schmidt and the Sorbonne will fund a new A.I. program to <a href="https://www.nytimes.com/2025/03/26/arts/design/digital-delacroix-ai-eric-wendy-schmidt-murals.html">digitize the master of Romanticism’s papers</a> and identify other artists who may have contributed to his murals and paintings.</p></li><li><p><strong>New Uses for Old Drugs: </strong>Scientists are using machine learning to <a href="https://www.nytimes.com/2025/03/20/well/ai-drug-repurposing.html">find new treatments</a> among thousands of old medicines.</p></li></ul></section></div></div><div id="bottom-sheet-sensor"></div><div><div id="bottom-wrapper"><p>Advertisement</p><p><a href="#after-bottom">SKIP ADVERTISEMENT</a></p></div></div></article></div></div></div>
  </body>
</html>
