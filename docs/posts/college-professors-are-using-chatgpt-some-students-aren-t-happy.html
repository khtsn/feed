<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>College Professors Are Using ChatGPT. Some Students Aren’t Happy. | Khanh's feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nytimes.com/2025/05/14/technology/chatgpt-college-professors.html">Original</a>
    <h1>College Professors Are Using ChatGPT. Some Students Aren’t Happy.</h1>
    
    <div id="readability-page-1" class="page"><div id="site-content"><div><article id="story"><header><p id="article-summary">Students call it hypocritical. A senior at Northeastern University demanded her tuition back. But instructors say generative A.I. tools make them better at their jobs.</p><div data-testid="imageblock-wrapper"><figure aria-label="media" role="group"><div data-testid="imageContainer-children-Image"><picture><source media="(max-width: 599px) and (min-device-pixel-ratio: 3),(max-width: 599px) and (min-resolution: 3dppx),(max-width: 599px) and (min-resolution: 288dpi)" srcset="https://static01.nyt.com/images/2025/05/14/multimedia/14Biz-AIProfessor-01-wklz/14Biz-AIProfessor-01-wklz-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1800"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 2),(max-width: 599px) and (min-resolution: 2dppx),(max-width: 599px) and (min-resolution: 192dpi)" srcset="https://static01.nyt.com/images/2025/05/14/multimedia/14Biz-AIProfessor-01-wklz/14Biz-AIProfessor-01-wklz-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1200"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 1),(max-width: 599px) and (min-resolution: 1dppx),(max-width: 599px) and (min-resolution: 96dpi)" srcset="https://static01.nyt.com/images/2025/05/14/multimedia/14Biz-AIProfessor-01-wklz/14Biz-AIProfessor-01-wklz-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=600"/><img alt="Ella Stapleton, in a black sweater and jeans, sits on a couch in front of a colorful wall hanging." src="https://static01.nyt.com/images/2025/05/14/multimedia/14Biz-AIProfessor-01-wklz/14Biz-AIProfessor-01-wklz-articleLarge.jpg?quality=75&amp;auto=webp&amp;disable=upscale" srcset="https://static01.nyt.com/images/2025/05/14/multimedia/14Biz-AIProfessor-01-wklz/14Biz-AIProfessor-01-wklz-articleLarge.jpg?quality=75&amp;auto=webp 600w,https://static01.nyt.com/images/2025/05/14/multimedia/14Biz-AIProfessor-01-wklz/14Biz-AIProfessor-01-wklz-jumbo.jpg?quality=75&amp;auto=webp 683w,https://static01.nyt.com/images/2025/05/14/multimedia/14Biz-AIProfessor-01-wklz/14Biz-AIProfessor-01-wklz-superJumbo.jpg?quality=75&amp;auto=webp 1366w" sizes="((min-width: 600px) and (max-width: 1004px)) 84vw, (min-width: 1005px) 80vw, 100vw" decoding="async" width="600" height="900"/></picture></div><figcaption data-testid="photoviewer-children-ImageCaption"><span>Ella Stapleton said she was surprised to find that a professor had used ChatGPT to assemble course materials. “He’s telling us not to use it, and then he’s using it himself,” she said.</span><span><span>Credit...</span><span><span aria-hidden="false">Oliver Holms for The New York Times</span></span></span></figcaption></figure></div><p><time datetime="2025-05-14T05:00:36-04:00">May 14, 2025, <span>5:00 a.m. ET</span></time></p></header><section name="articleBody"><div data-testid="companionColumn-0"><div><p>In February, Ella Stapleton, then a senior at Northeastern University, was reviewing lecture notes from her organizational behavior class when she noticed something odd. Was that a query to ChatGPT from her professor?</p><p>Halfway through the document, which her business professor had made for a lesson on models of leadership, was an instruction to ChatGPT to “expand on all areas. Be more detailed and specific.” It was followed by a list of positive and negative leadership traits, each with a prosaic definition and a bullet-pointed example.</p><p>Ms. Stapleton texted a friend in the class.</p><p>“Did you see the notes he put on Canvas?” she wrote, referring to the university’s software platform for hosting course materials. “He made it with ChatGPT.”</p><p>“OMG Stop,” the classmate responded. “What the hell?”</p><p>Ms. Stapleton decided to do some digging. She reviewed her professor’s slide presentations and discovered other telltale signs of A.I.: distorted text, photos of office workers with extraneous body parts and egregious misspellings.</p></div></div><div data-testid="companionColumn-1"><div><p>She was not happy. Given the school’s cost and reputation, she expected a top-tier education. This course was required for her business minor; its syllabus forbade “academically dishonest activities,” including the unauthorized use of artificial intelligence or chatbots.</p><p>“He’s telling us not to use it, and then he’s using it himself,” she said.</p><p>Ms. Stapleton filed a formal complaint with Northeastern’s business school, citing the undisclosed use of A.I. as well as other issues she had with his teaching style, and requested reimbursement of tuition for that class. As a quarter of the total bill for the semester, that would be more than $8,000.</p></div></div><div data-testid="ImageBlock-3"><div data-testid="imageblock-wrapper"><figure aria-label="media" role="group"><div data-testid="photoviewer-children-figure"><p><span>Image</span></p><div data-testid="lazy-image"></div></div><figcaption data-testid="photoviewer-children-caption"><span>A slide presentation for an organizational behavior class that Ms. Stapleton took contained misspellings.</span><span><span>Credit...</span><span><span aria-hidden="false">Ella Stapleton</span></span></span></figcaption></figure></div></div><div data-testid="companionColumn-2"><div><p>When ChatGPT was released at the end of 2022, it caused <a href="https://www.nytimes.com/2023/01/16/technology/chatgpt-artificial-intelligence-universities.html" title="">a panic</a> at all levels of education because it made cheating <a href="https://nymag.com/intelligencer/article/openai-chatgpt-ai-cheating-education-college-students-school.html" title="" rel="noopener noreferrer" target="_blank">incredibly easy</a>. Students who were asked to write a history paper or literary analysis could have the tool do it in mere seconds. Some schools banned it while others deployed A.I. detection services, despite concerns about <a href="https://www.vanderbilt.edu/brightspace/2023/08/16/guidance-on-ai-detection-and-why-were-disabling-turnitins-ai-detector/" title="" rel="noopener noreferrer" target="_blank">their accuracy</a>.</p><p>But, oh, how the tables have turned. Now students are complaining on sites like Rate My Professors about their instructors’ overreliance on A.I. and scrutinizing course materials for words ChatGPT tends to overuse, like “crucial” and “delve.” In addition to calling out hypocrisy, they make a financial argument: They are paying, often quite a lot, to be taught by humans, not an algorithm that they, too, could consult for free.</p></div></div><div data-testid="companionColumn-3"><div><p>For their part, professors said they used A.I. chatbots as a tool to provide a better education. Instructors interviewed by The New York Times said chatbots saved time, helped them with overwhelming workloads and served as automated teaching assistants.</p><p>Their numbers are growing. In <a href="https://tytonpartners.com/time-for-class-2024/" title="" rel="noopener noreferrer" target="_blank">a national survey</a> of more than 1,800 higher-education instructors last year, 18 percent described themselves as frequent users of generative A.I. tools; in a repeat survey this year, that percentage nearly doubled, according to Tyton Partners, the consulting group that conducted the research. The A.I. industry wants to help, and to profit: The start-ups <a href="https://openai.com/chatgpt/education/" title="" rel="noopener noreferrer" target="_blank">OpenAI</a> and <a href="https://www.anthropic.com/education" title="" rel="noopener noreferrer" target="_blank">Anthropic</a> recently created enterprise versions of their chatbots designed for universities.</p><p>(The Times has <a href="https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html" title="">sued</a> OpenAI for copyright infringement for use of news content without permission.)</p><p>Generative A.I. is clearly here to stay, but universities are struggling to keep up with the changing norms. Now professors are the ones on the learning curve and, like Ms. Stapleton’s teacher, muddling their way through the technology’s pitfalls and their students’ disdain.</p><h2 id="link-766b3682">Making the Grade</h2><p>Last fall, Marie, 22, wrote a three-page essay for an online anthropology course at Southern New Hampshire University. She looked for her grade on the school’s online platform, and was happy to have received an A. But in a section for comments, her professor had accidentally posted a back-and-forth with ChatGPT. It included the grading rubric the professor had asked the chatbot to use and a request for some “really nice feedback” to give Marie.</p><p>“From my perspective, the professor didn’t even read anything that I wrote<em>,</em>” said Marie, who asked to use her middle name and requested that her professor’s identity not be disclosed. She could understand the temptation to use A.I. Working at the school was a “third job” for many of her instructors, who might have hundreds of students, said Marie, and she did not want to embarrass her teacher.</p></div></div><div data-testid="companionColumn-4"><div><p>Still, Marie felt wronged and confronted her professor during a Zoom meeting. The professor told Marie that she did read her students’ essays but used ChatGPT as a guide, which the school permitted.</p><p>Robert MacAuslan, vice president of A.I. at Southern New Hampshire, said that the school believed “in the power of A.I. to transform education” and that there were guidelines for both faculty and students to “ensure that this technology enhances, rather than replaces, human creativity and oversight.” A <a href="https://snhu-externalaffairs.app.box.com/s/qwe7nm71tvqg18fmii3heqy45cuqgx1b" title="" rel="noopener noreferrer" target="_blank">dos and don’ts</a> for faculty forbids using tools, such as ChatGPT and Grammarly, “in place of authentic, human-centric feedback.”</p><p>“These tools should never be used to ‘do the work’ for them,” Dr. MacAuslan said. “Rather, they can be looked at as enhancements to their already established processes.”</p><p>After a second professor appeared to use ChatGPT to give her feedback, Marie transferred to another university.</p><p>Paul Shovlin, an English professor at Ohio University in Athens, Ohio, said he could understand her frustration. “Not a big fan of that,” Dr. Shovlin said, after being told of Marie’s experience. Dr. Shovlin is also an <a href="https://www.ohio.edu/center-teaching-learning/teaching-learning-genai" title="" rel="noopener noreferrer" target="_blank">A.I. faculty fellow</a>, whose role includes developing the right ways to incorporate A.I. into teaching and learning.</p></div></div><div data-testid="companionColumn-5"><p>“The value that we add as instructors is the feedback that we’re able to give students,” he said<em>. “</em>It’s the human connections that we forge with students as human beings who are reading their words and who are being impacted by them.”</p></div><div data-testid="ImageBlock-11"><div data-testid="imageblock-wrapper"><figure aria-label="media" role="group"><div data-testid="photoviewer-children-figure"><p><span>Image</span></p><div data-testid="lazy-image"></div></div><figcaption data-testid="photoviewer-children-caption"><span>“The value that we add as instructors is the feedback that we’re able to give students,” said Paul Shovlin, a professor and A.I. faculty fellow at Ohio University.</span><span><span>Credit...</span><span><span aria-hidden="false">Rich-Joseph Facun for The New York Times</span></span></span></figcaption></figure></div></div><div data-testid="companionColumn-6"><div><p>Dr. Shovlin is a proponent of incorporating A.I. into teaching, but not simply to make an instructor’s life easier. Students need to learn to use the technology responsibly and “develop an ethical compass with A.I.,” he said, because they will almost certainly use it in the workplace. Failure to do so properly could have consequences. “If you screw up, you’re going to be fired,” Dr. Shovlin said.</p><p>One example he uses in his own classes: In 2023, officials at Vanderbilt University’s education school responded to a mass shooting at another university by sending <a href="https://t.e2ma.net/message/ul182h/m74zooz" title="" rel="noopener noreferrer" target="_blank">an email</a> to students calling for community cohesion. The message, which described promoting a “culture of care” by “building strong relationships with one another,” included a sentence at the end that revealed that <a href="https://www.cnn.com/2023/02/22/tech/vanderbilt-chatgpt-shooting-email/index.html" title="" rel="noopener noreferrer" target="_blank">ChatGPT had been used to write it</a>. After <a href="https://vanderbilthustler.com/2023/02/17/peabody-edi-office-responds-to-msu-shooting-with-email-written-using-chatgpt/" title="" rel="noopener noreferrer" target="_blank">students criticized</a> the outsourcing of empathy to a machine, the officials involved <a href="https://vanderbilthustler.com/2023/02/19/peabody-edi-deans-to-temporarily-step-back-following-chatgpt-crafted-message-about-msu-shooting/" title="" rel="noopener noreferrer" target="_blank">temporarily stepped down</a>.</p><p>Not all situations are so clear cut. Dr. Shovlin said it was tricky to come up with rules because reasonable A.I. use may vary depending on the subject. His department, the Center for Teaching, Learning and Assessment, instead has “<a href="https://www.ohio.edu/center-teaching-learning/teaching-learning-genai" title="" rel="noopener noreferrer" target="_blank">principles</a>” for A.I. integration, one of which eschews a “one-size-fits-all approach.”</p></div></div><div data-testid="companionColumn-7"><div><p>The Times contacted dozens of professors whose students had mentioned their A.I. use in online reviews. The professors said they had used ChatGPT to create computer science programming assignments and quizzes on required reading, even as students complained that the results didn’t always make sense. They used it to organize their feedback to students, or to make it kinder. As experts in their fields, they said, they can recognize when it hallucinates, or gets facts wrong.</p><p>There was no consensus among them as to what was acceptable. Some acknowledged using ChatGPT to help grade students’ work; others decried the practice. Some emphasized the importance of transparency with students when deploying generative A.I., while others said they didn’t disclose its use because of students’ skepticism about the technology.</p><p>Most, however, felt that Ms. Stapleton’s experience at Northeastern — in which her professor appeared to use A.I. to generate class notes and slides — was perfectly fine. That was Dr. Shovlin’s view, as long as the professor edited what ChatGPT spat out to reflect his expertise. Dr. Shovlin compared it to a longstanding practice in academia of using content, such as lesson plans and case studies, from third-party publishers.</p><p>To say a professor is “some kind of monster” for using A.I. to generate slides “is, to me, ridiculous,” he said.</p><h2 id="link-7de2886b">The Calculator on Steroids</h2><p>Shingirai Christopher Kwaramba, a business professor at Virginia Commonwealth University, described ChatGPT as a partner that saved time. Lesson plans that used to take days to develop now take hours, he said. He uses it, for example, to generate data sets for fictional chain stores, which students use in an exercise to understand various statistical concepts.</p></div></div><div data-testid="companionColumn-8"><div><p>“I see it as the age of the calculator on steroids,” Dr. Kwaramba said.</p><p>Dr. Kwaramba said he now had more time for student office hours.</p></div></div><div data-testid="ImageBlock-17"><div data-testid="imageblock-wrapper"><figure aria-label="media" role="group"><div data-testid="photoviewer-children-figure"><p><span>Image</span></p><div data-testid="lazy-image"></div></div><figcaption data-testid="photoviewer-children-caption"><span>The Harvard campus in Cambridge, Mass. One instructor there said he had integrated a custom chatbot into a computer programming class. </span><span><span>Credit...</span><span><span aria-hidden="false">Sophie Park for The New York Times</span></span></span></figcaption></figure></div></div><div data-testid="companionColumn-9"><div><p>Other professors, like David Malan at Harvard, said the use of A.I. meant fewer students were coming to office hours for remedial help. Dr. Malan, a computer science professor, has integrated a custom <a href="https://seas.harvard.edu/news/2024/01/quacking-computer-programming" title="" rel="noopener noreferrer" target="_blank">A.I. chatbot</a> into a popular class he teaches on the fundamentals of computer programming. His hundreds of students can turn to it for help with their coding assignments.</p><p>Dr. Malan has had to <a href="https://cs.harvard.edu/malan/publications/fp0627-liu.pdf" title="" rel="noopener noreferrer" target="_blank">tinker</a> with the chatbot to hone its pedagogical approach, so that it offers only guidance and not the full answers. The majority of 500 students surveyed in 2023, the first year it was offered, said they found it <a href="https://cs.harvard.edu/malan/publications/V1fp0567-liu.pdf" title="" rel="noopener noreferrer" target="_blank">helpful</a>.</p><p>Rather than spend time on “more mundane questions about introductory material” during office hours, he and his teaching assistants prioritize interactions with students at weekly lunches and hackathons — “more memorable moments and experiences,” Dr. Malan said.</p></div></div><div data-testid="companionColumn-10"><div><p>Katy Pearce, a communication professor at the University of Washington, developed a custom A.I. chatbot by training it on versions of old assignments that she had graded. It can now give students feedback on their writing that mimics her own at any time, day or night. It has been beneficial for students who are otherwise hesitant to ask for help, she said.</p><p>“Is there going to be a point in the foreseeable future that much of what graduate student teaching assistants do can be done by A.I.?” she said. “Yeah, absolutely.”</p><p>What happens then to the pipeline of future professors who would come from the ranks of teaching assistants?</p><p>“It will absolutely be an issue,” Dr. Pearce said.</p><h2 id="link-6e57e60f">A Teachable Moment</h2><p>After filing her complaint at Northeastern, Ms. Stapleton had a series of meetings with officials in the business school. In May, the day after her graduation ceremony, the officials told her that she was not getting her tuition money back.</p><p>Rick Arrowood, her professor, was contrite about the episode. Dr. Arrowood, who is an adjunct professor and has been teaching for nearly two decades, said he had uploaded his class files and documents to ChatGPT, the A.I. search engine Perplexity and an A.I. presentation generator called Gamma to “give them a fresh look.” At a glance, he said, the notes and presentations they had generated looked great.</p></div></div><div data-testid="companionColumn-11"><div><p>“In hindsight, I wish I would have looked at it more closely,” he said.</p><p>He put the materials online for students to review, but emphasized that he did not use them in the classroom, because he prefers classes to be discussion-oriented. He realized the materials were flawed only when school officials questioned him about them.</p><p>The embarrassing situation made him realize, he said, that professors should approach A.I. with more caution and disclose to students when and how it is used. Northeastern issued a formal A.I. policy only recently; <a href="https://policies.northeastern.edu/policy125/" title="" rel="noopener noreferrer" target="_blank">it requires attribution</a> when A.I. systems are used and review of the output for “accuracy and appropriateness.” A Northeastern spokeswoman said the school “embraces the use of artificial intelligence to enhance all aspects of its teaching, research and operations.”</p><p>“I’m all about teaching,” Dr. Arrowood said. “If my experience can be something people can learn from, then, OK, that’s my happy spot.”</p></div></div></section><div><div><div><p>Kashmir Hill writes about technology and how it is changing people’s everyday lives with a particular focus on privacy. She has been covering technology for more than a decade.</p></div></div></div><div><div><section id="styln-guide" role="complementary" aria-labelledby="styln-guide-title"><hr/><h2 id="styln-guide-title">Explore Our Coverage of Artificial Intelligence</h2><hr/><p><strong>News</strong><strong> and Analysis</strong></p><ul><li><p><strong>Trump’s Middle East Visit: </strong>As President Trump heads to the region, America’s dominance over A.I. chips has become a <a href="https://www.nytimes.com/2025/05/12/business/economy/trump-middle-east-trip-semiconductor-deals.html">powerful source of leverage for the president</a>.</p></li><li><p><strong>Google’s A.I. Chatbot: </strong>The tech giant said it would make its Gemini chatbot <a href="https://www.nytimes.com/2025/05/02/technology/google-gemini-ai-chatbot-kids.html">available to children</a>, and warned families in an email about the changes.</p></li><li><p><strong>Venture Capital:</strong> Thrive Capital has bet big on artificial intelligence, including emerging giants of the field like OpenAI and Databricks. Now the venture capital firm is taking a different approach: <a href="https://www.nytimes.com/2025/04/29/business/dealbook/thrive-holdings-rollup-ai.html">creating and buying companies</a> that it believes can benefit from A.I.</p></li><li><p><strong>OpenAI&#39;s Plan B</strong><strong>:</strong> OpenAI’s decision to <a href="https://www.nytimes.com/2025/05/06/business/dealbook/altman-openai-plan-b.html">scale back its ambitious corporate reorganization</a> has drawn lots of scrutiny, including what the plan means for artificial intelligence safety, potential profits for investors and an ongoing fight with Elon Musk.</p></li></ul><hr/><p><strong>The Age of A.I.</strong></p><ul><li><p><strong>An Agatha Christie Avatar: </strong>The long-dead British novelist has been reanimated and is “teaching” an online writing course. <a href="https://www.nytimes.com/2025/05/08/world/europe/agatha-christie-ai-class-bbc.html">But do we want to learn from a digital prosthetic</a> built by artificial intelligence?</p></li><li><p><strong>Writing Tools on Your Phone: </strong>Artificial intelligence software — some already free on your device — can quickly compose and edit documents. <a href="https://www.nytimes.com/2025/05/07/technology/personaltech/ai-writing-tools-phone.html">But be sure to check its work</a>.</p></li><li><p><strong>A.I. Can Trick You, Warns Book That Hid A.I.’s Help Writing It:</strong> People were deceived. Reacting to accusations of dishonesty and even illegality,the man behind the book defends it, <a href="https://www.nytimes.com/2025/04/30/world/europe/hypnocracy-ai-philosopher-book.html">calling it not a prank but a “philosophical experiment</a>.”</p></li></ul></section></div></div></article></div></div></div>
  </body>
</html>
