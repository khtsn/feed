<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>That Message From Your Doctor? It May Have Been Drafted by A.I. | Khanh's feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nytimes.com/2024/09/24/health/ai-patient-messages-mychart.html">Original</a>
    <h1>That Message From Your Doctor? It May Have Been Drafted by A.I.</h1>
    
    <div id="readability-page-1" class="page"><article id="story"><div id="top-wrapper"><p>Advertisement</p><p><a href="#after-top">SKIP ADVERTISEMENT</a></p><div></div></div><header><div id="sponsor-wrapper"><p>Supported by</p><p><a href="#after-sponsor">SKIP ADVERTISEMENT</a></p></div><p id="article-summary">Overwhelmed by queries, physicians are turning to artificial intelligence to correspond with patients. Many have no clue that the replies are software-generated.</p><div><div role="toolbar" data-testid="share-tools" aria-label="Social Media Share buttons, Save button, and Comments Panel with current comment count"><div><ul><li><div></div></li><li><div></div></li><li></li></ul></div></div></div><div data-testid="imageblock-wrapper"><figure aria-label="media" role="group"><div data-testid="imageContainer-children-Image"><picture><source media="(max-width: 599px) and (min-device-pixel-ratio: 3),(max-width: 599px) and (-webkit-min-device-pixel-ratio: 3),(max-width: 599px) and (min-resolution: 3dppx),(max-width: 599px) and (min-resolution: 288dpi)" srcset="https://static01.nyt.com/images/2024/09/19/multimedia/00ai-mychart-01-gmkh/00ai-mychart-01-gmkh-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=600"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 2),(max-width: 599px) and (-webkit-min-device-pixel-ratio: 2),(max-width: 599px) and (min-resolution: 2dppx),(max-width: 599px) and (min-resolution: 192dpi)" srcset="https://static01.nyt.com/images/2024/09/19/multimedia/00ai-mychart-01-gmkh/00ai-mychart-01-gmkh-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1200"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 1),(max-width: 599px) and (-webkit-min-device-pixel-ratio: 1),(max-width: 599px) and (min-resolution: 1dppx),(max-width: 599px) and (min-resolution: 96dpi)" srcset="https://static01.nyt.com/images/2024/09/19/multimedia/00ai-mychart-01-gmkh/00ai-mychart-01-gmkh-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1800"/><img alt="A screenshot of the MyChart website shows a hand holding up a smartphone with a graph indicating a patient&#39;s blood pressure. The text reads, &#34;All your health information in one place&#34; and is followed by a paragraph about the benefits of using MyChart." src="https://static01.nyt.com/images/2024/09/19/multimedia/00ai-mychart-01-gmkh/00ai-mychart-01-gmkh-articleLarge.jpg?quality=75&amp;auto=webp&amp;disable=upscale" srcset="https://static01.nyt.com/images/2024/09/19/multimedia/00ai-mychart-01-gmkh/00ai-mychart-01-gmkh-articleLarge.jpg?quality=75&amp;auto=webp 600w,https://static01.nyt.com/images/2024/09/19/multimedia/00ai-mychart-01-gmkh/00ai-mychart-01-gmkh-jumbo.jpg?quality=75&amp;auto=webp 1024w,https://static01.nyt.com/images/2024/09/19/multimedia/00ai-mychart-01-gmkh/00ai-mychart-01-gmkh-superJumbo.jpg?quality=75&amp;auto=webp 1768w" sizes="((min-width: 600px) and (max-width: 1004px)) 84vw, (min-width: 1005px) 80vw, 100vw" decoding="async" width="600" height="331"/></picture></div><figcaption data-testid="photoviewer-children-ImageCaption"><span>MyChart, the widely used patient portal software, now has a tool powered by A.I. that doctors can use to compose messages to their patients.</span><span><span>Credit...</span><span><span aria-hidden="false">MyChart</span></span></span></figcaption></figure></div><div><div><p><a href="https://www.nytimes.com/by/teddy-rosenbluth"><img alt="Teddy Rosenbluth" title="Teddy Rosenbluth" src="https://static01.nyt.com/images/2024/08/21/reader-center/author-teddy-rosenbluth/author-teddy-rosenbluth-thumbLarge.png"/></a></p><div><p><a href="https://www.nytimes.com/by/teddy-rosenbluth" itemprop="name">Teddy Rosenbluth</a></p></div></div></div><p><time datetime="2024-09-24T10:08:59-04:00">Sept. 24, 2024, <span>10:08 a.m. ET</span></time></p></header><section name="articleBody"><div data-testid="companionColumn-0"><div><p>Every day, patients send hundreds of thousands of messages to their doctors through MyChart, a communications platform that is nearly ubiquitous in U.S. hospitals.</p><p>They describe their pain and divulge their symptoms — the texture of their rashes, the color of their stool — trusting the doctor on the other end<strong> </strong>to advise them. </p><p>But increasingly, the responses to those messages are not written by the doctor — at least, not entirely. About 15,000 doctors and assistants at more than 150 health systems are using a new artificial intelligence feature in MyChart to draft replies to such messages.</p><p>Many patients receiving those replies have no idea that they were written with the help of artificial intelligence. In interviews, officials at several health systems using MyChart’s tool acknowledged that they do not disclose that the messages contain A.I.-generated content.</p></div></div><div data-testid="companionColumn-1"><div><p>The trend troubles some experts who worry that doctors may not be vigilant enough to catch potentially dangerous errors in medically significant messages drafted by A.I.</p><p>In an industry that has largely used A.I. to tackle administrative tasks like <a href="https://www.nytimes.com/2023/06/26/technology/ai-health-care-documentation.html" title="">summarizing appointment notes</a> or <a href="https://www.nytimes.com/2024/07/10/health/doctors-insurers-artificial-intelligence.html" title="">appealing insurance denials</a>, critics fear that the wide adoption of MyChart’s tool has allowed A.I. to edge into clinical decision-making and doctor-patient relationships.</p><p>Already the tool can be instructed to write in an individual doctor’s voice. But it does not always draft correct responses.</p><p>“The sales pitch has been that it’s supposed to save them time so that they can spend more time talking to patients,” said Athmeya Jayaram, a researcher at the Hastings Center, a bioethics research institute in Garrison, N.Y.</p><p>“In this case, they’re trying to save time talking to patients with generative A.I.”</p><p>During the peak of the pandemic, when in-person appointments were often reserved for the sickest patients, many turned to MyChart messages as a rare, direct line of communication with their doctors.</p></div></div><div data-testid="companionColumn-2"><div><p>It wasn’t until years later that providers realized they had a problem: Even after most aspects of health care returned to normal, they were still swamped with patient messages.</p><p>Already overburdened doctors were suddenly spending lunch breaks and evenings replying to patient messages. Hospital leaders feared that if they didn’t find a way to reduce this extra — largely nonbillable — work, the patient messages would become a major driver of physician burnout.</p></div></div><div data-testid="ImageBlock-5"><div data-testid="imageblock-wrapper"><figure aria-label="media" role="group"><div data-testid="photoviewer-children-figure"><p><span>Image</span></p><div data-testid="lazy-image"></div></div><figcaption data-testid="photoviewer-children-caption"><span>More than a hundred doctors at U.W. Health now have access to an A.I. tool to draft patient messages.</span><span><span>Credit...</span><span><span aria-hidden="false">Jamie Kelter Davis for The New York Times</span></span></span></figcaption></figure></div></div><div data-testid="companionColumn-3"><div><p>So in early 2023, when Epic, the software giant that developed MyChart, began offering a new tool that used A.I. to compose replies, some of the country’s largest academic medical centers were eager to adopt it.</p><p>Instead of starting each message with a blank screen, a doctor sees an automatically generated response above the patient’s question. The response is created with a version of GPT-4 (the technology underlying ChatGPT) that complies with medical privacy laws.</p></div></div><div data-testid="companionColumn-4"><div><p>The MyChart tool, called In Basket Art, pulls in context from the patient’s prior messages and information from his or her electronic medical records, like a medication list, to create a draft that providers can approve or change.</p><p>By letting doctors act more like editors, health systems hoped they could get through their patient messages faster and spend less mental energy doing it.</p><p>This has been partially borne out in early studies, which have found that Art did lessen <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10955355/" title="" rel="noopener noreferrer" target="_blank">feelings of burnout</a> and <a href="https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocae172/7717344" title="" rel="noopener noreferrer" target="_blank">cognitive burden</a>, but did not necessarily save time.</p><p>Several hundred clinicians at U.C. San Diego Health, more than a hundred providers at U.W. Health in Wisconsin and every licensed clinician at Stanford Health Care’s primary care practices — including doctors, nurses and pharmacists — have access to the A.I. tool.</p><p>Dozens of doctors at Northwestern Health, N.Y.U. Langone Health and U.N.C. Health are piloting Art while leaders consider a broader expansion.</p></div></div><div data-testid="companionColumn-5"><div><p>In the absence of strong <a href="https://www.nytimes.com/2023/10/30/health/doctors-ai-technology-health-care.html" title="">federal regulations</a> or widely accepted ethical frameworks, each health system decides how to test the tool’s safety and whether to inform patients about its use.</p><p>Some hospital systems, like U.C. San Diego Health, put a disclosure at the bottom of each message explaining that it has been “generated automatically,” and reviewed and edited by a physician.</p><p>“I see, personally, no downside to being transparent,” said Dr. Christopher Longhurst, the health system’s chief clinical and innovation officer.</p><p>Patients have generally accepted the new technology, he said. (One doctor received an email saying, “I<strong> </strong>want to be the first to congratulate you on your A.I. co-pilot and be the first to send you an A.I.-generated patient message.”)</p><p>Other systems — including Stanford Health Care, U.W. Health, U.N.C. Health and N.Y.U. Langone Health — decided that notifying patients would do more harm than good.</p></div></div><div data-testid="companionColumn-6"><div><p>Some administrators worried that doctors might see the disclaimer as an excuse to send messages to patients without properly vetting them, said Dr. Brian Patterson, the physician administrative director for clinical A.I. at U.W. Health.</p><p>And telling patients the message had A.I. content might cheapen the clinical advice, even though it was endorsed by their physicians, said Dr. Paul Testa, chief medical information officer at NYU Langone Health.</p></div></div><div data-testid="ImageBlock-13"><div data-testid="imageblock-wrapper"><figure aria-label="media" role="group"><div data-testid="photoviewer-children-figure"><p><span>Image</span></p><div data-testid="lazy-image"></div></div><figcaption data-testid="photoviewer-children-caption"><span>The headquarters of Epic Systems Corporation, which makes the MyChart software used by hospitals across the United States.</span><span><span>Credit...</span><span><span aria-hidden="false">Narayan Mahon for The New York Times</span></span></span></figcaption></figure></div></div><div data-testid="companionColumn-7"><div><p>To Dr. Jayaram, whether to disclose use of the tool comes down to a simple question: What do patients expect?</p><p>When patients send a message about their health, he said, they assume that their doctors will consider their history, treatment preferences and family dynamics — intangibles gleaned from longstanding relationships.</p></div></div><div data-testid="companionColumn-8"><div><p>“When you read a doctor’s note, you read it in the voice of your doctor,” he said. “If a patient were to know that, in fact, the message that they’re exchanging with their doctor is generated by A.I., I think they would feel rightly betrayed.”</p><p>To many health systems, creating an algorithm that convincingly mimics a particular physician’s “voice” helps make the tool useful. Indeed, Epic recently started giving its tool greater access to past messages, so that its drafts could imitate each doctor’s individual writing style.</p><p>Brent Lamm, deputy chief information officer for U.N.C. Health, said this addressed common complaints he heard from doctors: “My personal voice is not coming through” or “I’ve known this patient for seven years. They’re going to know it’s not me.”</p><p>Health care administrators often refer to Art as a low-risk use of A.I., since ideally a provider is always reading through the drafts and correcting mistakes.</p><p>The characterization annoys researchers who study how humans work in relation to artificial intelligence. Ken Holstein, a professor at the human-computer interaction institute at Carnegie Mellon, said the portrayal “goes against about 50 years of research.”</p></div></div><div data-testid="companionColumn-9"><div><p>Humans have a well-documented tendency, called automation bias, to accept an algorithm’s recommendations even if it contradicts their own expertise, he said. This bias could cause doctors to be less critical while reviewing A.I.-generated drafts, potentially allowing dangerous errors to reach patients.</p><p>And Art is not immune to mistakes. A <a href="https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocae172/7717344" title="" rel="noopener noreferrer" target="_blank">recent study</a> found that seven of 116 A.I.-generated drafts contained so-called hallucinations — fabrications that the technology is <a href="https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html" title="">notorious for</a> conjuring.</p><p>Dr. Vinay Reddy, a family medicine doctor at U.N.C. Health, recalled an instance in which a patient messaged a colleague to check whether she needed a hepatitis B vaccine.</p><p>The A.I.-generated draft confidently assured the patient she had gotten her shots and provided dates for them. This was completely false, and occurred because the model didn’t have access to her vaccination records, he said.</p><p>A small <a href="https://www.thelancet.com/journals/landig/article/PIIS2589-7500(24)00060-8/fulltext" title="" rel="noopener noreferrer" target="_blank">study published in The Lancet Digital Health</a> found that GPT-4, the same A.I. model that underlies Epic’s tool, made more insidious errors when answering hypothetical patient questions.</p></div></div><div data-testid="companionColumn-10"><div><p>Physicians reviewing its answers found that the drafts, if left unedited, would pose a risk of severe harm about 7 percent of the time.</p><p>What reassures Dr. Eric Poon, chief health information officer at Duke Health, is that the model produces drafts that are still “moderate in quality,” which he thinks keeps doctors skeptical and vigilant about catching errors.</p></div></div><div data-testid="ImageBlock-21"><div data-testid="imageblock-wrapper"><figure aria-label="media" role="group"><div data-testid="photoviewer-children-figure"><p><span>Image</span></p><div data-testid="lazy-image"></div></div><figcaption data-testid="photoviewer-children-caption"><span>NYU Langone Health is one of several health systems piloting MyChart’s A.I. messenger and considering broader expansion.</span><span><span>Credit...</span><span><span aria-hidden="false">Gabby Jones for The New York Times</span></span></span></figcaption></figure></div></div><div data-testid="companionColumn-11"><div><p>On average, fewer than a third of A.I.-generated drafts are sent to patients unedited, according to Epic, an indication to hospital administrators that doctors are not rubber-stamping messages.</p><p>“One question in the back of my mind is, what if the technology got better?” he said. “What if clinicians start letting their guard down? Will errors slip through?”</p></div></div><div data-testid="companionColumn-12"><div><p>Epic has built guardrails into the programming to steer Art away from giving clinical advice, said Garrett Adams, a vice president of research and development at the company. </p><p>Mr. Adams said the tool was best suited to answer common administrative questions like “When is my appointment?” or “Can I reschedule my checkup?”</p><p>But researchers have not developed ways to reliably force the models to follow instructions, Dr. Holstein said.</p><p>Dr. Anand Chowdhury, who helped oversee deployment of Art at Duke Health, said he and his colleagues repeatedly adjusted instructions to stop the tool from giving clinical advice, with little success.</p><p>“No matter how hard we tried, we couldn’t take out its instinct to try to be helpful,” he said.</p><p>Three health systems told The New York Times that they removed some guardrails from the instructions.</p></div></div><div data-testid="companionColumn-13"><div><p>Dr. Longhurst at U.C. San Diego Health said the model “performed better” when language that instructed Art not to “respond with clinical information” was removed. Administrators felt comfortable giving the A.I. more freedom since doctors review its messages.</p><p>Stanford Health Care took a “managed risk” to allow Art to “think more like a clinician,” after some of the strictest guardrails seemed to make its drafts generic and unhelpful, said Dr. Christopher Sharp, the health system’s chief medical information officer.</p><p>Beyond questions of safety and transparency, some bioethicists have a more fundamental concern: Is this how we want to use A.I. in medicine?</p><p>Unlike many other A.I. health care tools, Art isn’t designed to improve clinical outcomes (though one study suggested responses may be more <a href="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2821167?utm_source=For_The_Media&amp;utm_medium=referral&amp;utm_campaign=ftm_links&amp;utm_term=071624" title="" rel="noopener noreferrer" target="_blank">empathetic and positive</a>), and it isn’t targeting strictly administrative tasks.</p><p>Instead, A.I. seems to be intruding on rare moments when patient and doctors could actually be communicating with one another directly — the kind of moments that technology should be enabling, said Daniel Schiff, co-director of the Governance and Responsible A.I. Lab at Purdue University.</p><p>“Even if it was flawless, do you want to automate one of the few ways that we’re still interacting with each other?<em>”</em></p></div></div></section><div><div><div><div><p>Teddy Rosenbluth is a health reporter and a member of the 2024-25 <a href="https://www.nytco.com/careers/newsroom/newsroom-fellowship/">Times Fellowship</a> class, a program for journalists early in their careers.<span> <a href="https://www.nytimes.com/by/teddy-rosenbluth">More about Teddy Rosenbluth</a></span></p></div></div></div><div role="toolbar" data-testid="share-tools" aria-label="Social Media Share buttons, Save button, and Comments Panel with current comment count"><div><ul><li><div></div></li><li><div></div></li><li></li></ul></div></div></div><div><div><section id="styln-guide" role="complementary" aria-labelledby="styln-guide-title"><hr/><h2 id="styln-guide-title">Discover More in Health and Science</h2><hr/><ul><li><p><strong>Fighting for American Bats:</strong> Researchers have found <a href="https://www.nytimes.com/2024/09/17/science/bats-white-nose-syndrome.html">several promising ways to thwart a fungus</a> that has been causing deadly white-nose syndrome in bats.</p></li><li><p><strong>Breast Reduction:</strong> The surgery has become one of the most common cosmetic procedures in the United States. Are women asserting their independence or <a href="https://www.nytimes.com/2024/09/20/well/breast-reduction-trend.html">capitulating to yet another impossible standard of beauty</a>?</p></li><li><p><strong>The Role of Eelgrass:</strong> Along the Atlantic coast of the United States, eelgrass is declining quickly. <a href="https://www.nytimes.com/2024/09/17/science/eelgrass-climate-change-maine.html">The plant plays an important role in coastal environments</a>.</p></li><li><p><strong>How Pregnancy Changes the Brain: </strong>As hormones surge during pregnancy, some brain areas shrink in what scientists say may be a <a href="https://www.nytimes.com/2024/09/16/health/pregnancy-brain-changes.html">fine-tuning that helps mothers bond with and care for their babies</a>.</p></li><li><p><strong>Adderall’s Psychosis Risk:</strong> Psychosis and mania are each known side effects of stimulant medications. A new study published in The American Journal of Psychiatry <a href="https://www.nytimes.com/2024/09/12/well/mind/adderall-vyvanse-mania-psychosis-study.html">suggests that dosage may play a role</a>.</p></li></ul></section></div></div><div id="bottom-sheet-sensor"></div><div><div id="bottom-wrapper"><p>Advertisement</p><p><a href="#after-bottom">SKIP ADVERTISEMENT</a></p></div></div></article></div>
  </body>
</html>
