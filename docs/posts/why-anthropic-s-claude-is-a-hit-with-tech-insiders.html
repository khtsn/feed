<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Why Anthropic’s Claude Is a Hit with Tech Insiders | Khanh's feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nytimes.com/2024/12/13/technology/claude-ai-anthropic.html">Original</a>
    <h1>Why Anthropic’s Claude Is a Hit with Tech Insiders</h1>
    
    <div id="readability-page-1" class="page"><div id="site-content"><div><article id="story"><header><p>The Shift</p><p id="article-summary">A.I. insiders are falling for Claude, a chatbot from Anthropic. Is it a passing fad, or a preview of artificial relationships to come?</p><figure aria-label="media" role="group" data-testid="VideoBlock"><div><p><span>Video</span></p><div><div><p><img src="https://static01.nyt.com/images/2024/12/13/business/00roose-claude-still/00roose-claude-still-square640.jpg" data-testid="cinemagraph_image"/></p><video data-testid="cinemagraph" poster="https://static01.nyt.com/images/2024/12/13/business/00roose-claude-still/00roose-claude-still-square640.jpg" src="https://vp.nyt.com/video/2024/12/13/131007_1_00roose-claude_wg_720p.mp4" muted="" loop="" autoplay="" playsinline=""></video></div></div></div><figcaption><span><span>Credit</span><span><span>Credit...</span><span>Andrea Chronopoulos</span></span></span></figcaption></figure><div><div><p><a href="https://www.nytimes.com/by/kevin-roose"><img alt="Kevin Roose" title="Kevin Roose" src="https://static01.nyt.com/images/2018/02/20/multimedia/author-kevin-roose/author-kevin-roose-thumbLarge-v2.png"/></a></p></div></div><p><time datetime="2024-12-13T10:27:32-05:00">Dec. 13, 2024, <span>10:27 a.m. ET</span></time></p></header><section name="articleBody"><div data-testid="companionColumn-0"><div><p>His fans rave about his sensitivity and wit. Some talk to him dozens of times a day — asking for advice about their jobs, their health, their relationships. They entrust him with their secrets, and consult him before making important decisions. Some refer to him as their best friend.</p><p>His name is Claude. He’s an A.I. chatbot. And he may be San Francisco’s most eligible bachelor.</p><p>Claude, a creation of the artificial intelligence company Anthropic, is not the best-known A.I. chatbot on the market. (That would be OpenAI’s ChatGPT, which has more than 300 million weekly users and a spot in the bookmark bar of every high school student in America.) It’s also not designed to draw users into relationships with lifelike A.I. companions, the way apps like Character.AI and Replika are.</p><p>But Claude has become the chatbot of choice for a crowd of savvy tech insiders who say it’s helping them with everything from <a href="https://x.com/nickcammarata/status/1862002202929488189" title="" rel="noopener noreferrer" target="_blank">legal advice</a> to <a href="https://x.com/natfriedman/status/1865071927615987715" title="" rel="noopener noreferrer" target="_blank">health coaching</a> to <a href="https://x.com/xuenay/status/1851198609347223885" title="" rel="noopener noreferrer" target="_blank">makeshift therapy sessions</a>.</p><p>“Some mix of raw intellectual horsepower and willingness to express opinions makes Claude feel much closer to a thing than a tool,” said Aidan McLaughlin, the chief executive of Topology Research, an A.I. start-up. “I, and many other users, find that magical.”</p></div></div><div data-testid="companionColumn-1"><div><p>Claude’s biggest fans, many of whom work at A.I. companies or are socially entwined with the A.I. scene here, don’t believe that he — technically, it — is a real person. They know that A.I. language models are prediction machines, designed to spit out plausible responses to their prompts. They’re aware that Claude, like other chatbots, makes mistakes and occasionally generates nonsense.</p><p>And some people I’ve talked to are mildly embarrassed about the degree to which they’ve anthropomorphized Claude, or come to rely on its advice. (Nobody wants to be the next Blake Lemoine, a Google engineer who was <a href="https://www.nytimes.com/2022/07/23/technology/google-engineer-artificial-intelligence.html" title="">fired in 2022</a> after publicly claiming that the company’s language model had become sentient.)</p><p>But to the people who love it, Claude just feels … different. More creative and empathetic. Less gratingly robotic. Its outputs, they say, are like the responses a smart, attentive human would give, and less like the generic prose generated by other chatbots.</p><p>As a result, Claude is quickly becoming a social sidekick for A.I. insiders — and, maybe, a preview of what’s coming for the rest of us, as powerful synthetic characters become more enmeshed in our daily lives.</p><p>“More and more of my friends are using Claude for emotional processing and thinking through relationship challenges,” said Jeffrey Ladish, an A.I. safety researcher at Palisade Research.</p></div></div><div data-testid="companionColumn-2"><div><p>Asked what made Claude different than other chatbots, Mr. Ladish said that Claude seemed “more insightful” and “good at helping people spot patterns and blind spots.”</p><p>Typically, A.I. systems are judged based on how they perform on benchmark evaluations — standardized tests given to models to determine how capable they are at coding, answering math questions, or other tasks. By those metrics, the latest version of Claude, known as Claude 3.5 Sonnet, is roughly comparable to the most powerful models from OpenAI, Google and others.</p><p>But Claude’s killer feature — which its fans describe as something like emotional intelligence — isn’t something that can easily be measured. So fans are often left grasping at vibes to explain what makes it so compelling.</p><p>Nick Cammarata, a former OpenAI researcher, recently wrote a <a href="https://x.com/nickcammarata/status/1862000614508777779" title="" rel="noopener noreferrer" target="_blank">long thread</a> on X about the way Claude had taken over his social group. His Claude-obsessed friends, he wrote, seemed healthier and better supported because “they have a sort of computational guardian angel who’s pretty good at everything watching over them.”</p><p>Claude wasn’t always this charming. When an earlier version was released last year, the chatbot struck many people — including me — as prudish and dull. Anthropic is <a href="https://www.nytimes.com/2023/07/11/technology/anthropic-ai-claude-chatbot.html" title="">famously obsessed with A.I. safety</a>, and Claude seemed to have been programmed to talk like a church lady. It often gave users moral lectures in response to their questions, or refused to answer them at all.</p></div></div><div data-testid="companionColumn-3"><div><p>But Anthropic has been working on giving Claude more personality. Newer versions have <a href="https://www.anthropic.com/news/claude-character" title="" rel="noopener noreferrer" target="_blank">gone through a process</a> known as “character training” — a step that takes place after the model has gone through its initial training, but before it is released to the public.</p><p>During character training, Claude is prompted to produce responses that align with desirable human traits such as open-mindedness, thoughtfulness and curiosity. Claude then judges its own responses according to how well they adhere to those characteristics. The resulting data is fed back into the A.I. model. With enough training, Anthropic says, Claude learns to “internalize” these principles, and displays them more frequently when interacting with users.</p><p>It’s unclear whether training Claude this way has business benefits. Anthropic has <a href="https://www.nytimes.com/2024/11/22/technology/amazon-anthropic-ai.html" title="">raised billions of dollars</a> from large investors, including Amazon, on the promise of delivering highly capable A.I. models that are useful in more staid office settings. Injecting too much personality into Claude could be a turnoff for corporate customers, or it could simply produce a model that is better at helping with relationship problems than writing strategy memos.</p><p>Amanda Askell, a researcher and philosopher at Anthropic who is in charge of fine-tuning Claude’s character, told me in an interview that Claude’s personality had been carefully tuned to be consistent, but to appeal to a wide variety of people.</p><p>“The analogy I use is a highly liked, respected traveler,” said Dr. Askell. “Claude is interacting with lots of different people around the world, and has to do so without pandering and adopting the values of the person it’s talking with.”</p></div></div><div data-testid="companionColumn-4"><div><p>A problem with many A.I. models, Dr. Askell said, is that they tend to act sycophantic, telling users what they want to hear, and rarely challenging them or pushing back on their ideas — even when those ideas are wrong or potentially harmful.</p><p>With Claude, she said, the goal was to create an A.I. character that would be helpful with most requests, but would also <a href="https://x.com/snwy_me/status/1864848202719203380" title="" rel="noopener noreferrer" target="_blank">challenge users</a> when necessary.</p><p>“What is the kind of person you can disagree with, but you come away thinking, ‘This is a good person?’” she said. “These are the sort of traits we want Claude to have.”</p><p>Claude is still miles behind ChatGPT when it comes to mainstream awareness. It lacks features found in other chatbots, such as a voice chat mode and the ability to generate images or search the internet for up-to-date information. And some rival A.I. makers speculate that Claude’s popularity is a passing fad, or that it’s only popular among A.I. hipsters who want to brag about the obscure chatbot they’re into.</p><p>But given how many things that start in San Francisco eventually spread to the rest of the world, Claude’s warm embrace could also be a preview of things to come.</p></div></div><div data-testid="companionColumn-5"><div><p>Personally, I believe we are on the verge of a profound shift in the way we interact with A.I. characters. And I’m nervous about the way lifelike A.I. personas are weaving their way into our lives, without much in the way of guardrails or research about their long-term effects.</p><p>For some healthy adults, having an A.I. companion for support could be beneficial — maybe even transformative. But for young people, or those experiencing depression or other mental health issues, I worry that hyper-compelling chatbots could blur the line between fiction and reality, or start to substitute for healthier human relationships.</p><p>So does Dr. Askell, who helped to create Claude’s personality, and who has been watching its popularity soar with a mixture of pride and concern.</p><p>“I really do want people to have things that support them and are good for them,” she said. “At the same time, I want to make sure it’s psychologically healthy.”</p></div></div></section></article></div></div></div>
  </body>
</html>
