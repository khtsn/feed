<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>This A.I. Forecast Predicts Storms Ahead | Khanh's feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nytimes.com/2025/04/03/technology/ai-futures-project-ai-2027.html">Original</a>
    <h1>This A.I. Forecast Predicts Storms Ahead</h1>
    
    <div id="readability-page-1" class="page"><div id="site-content"><div><article id="story"><section name="articleBody"><div data-testid="companionColumn-0"><div><p>The year is 2027. Powerful artificial intelligence systems are becoming smarter than humans, and are wreaking havoc on the global order. Chinese spies have stolen America’s A.I. secrets, and the White House is rushing to retaliate. Inside a leading A.I. lab, engineers are spooked to discover that their models are starting to deceive them, raising the possibility that they’ll go rogue.</p><p>These aren’t scenes from a sci-fi screenplay. They’re scenarios envisioned by a nonprofit in Berkeley, Calif., called the A.I. Futures Project, which has spent the past year trying to predict what the world will look like over the next few years, as increasingly powerful A.I. systems are developed.</p><p>The project is led by Daniel Kokotajlo, a former OpenAI researcher who <a href="https://www.nytimes.com/2024/06/04/technology/openai-culture-whistleblowers.html" title="">left the company last year</a> over his concerns that it was acting recklessly.</p><p>While at OpenAI, where he was on the governance team, Mr. Kokotajlo wrote detailed internal reports about how the race for artificial general intelligence, or A.G.I. — a fuzzy term for human-level machine intelligence — might unfold. After leaving, he teamed up with Eli Lifland, an A.I. researcher who had a <a href="https://www.vox.com/future-perfect/2024/2/13/24070864/samotsvety-forecasting-superforecasters-tetlock" title="" rel="noopener noreferrer" target="_blank">track record of accurately forecasting</a> world events. They got to work trying to predict A.I.’s next wave.</p></div></div><div data-testid="companionColumn-1"><div><p>The result is “AI 2027,” a report and website <a href="https://ai-2027.com/" title="" rel="noopener noreferrer" target="_blank">released this week</a> that describes, in a detailed fictional scenario, what could happen if A.I. systems surpass human-level intelligence — which the authors expect to happen in the next two to three years.</p><p>“We predict that A.I.s will continue to improve to the point where they’re fully autonomous agents that are better than humans at everything by the end of 2027 or so,” Mr. Kokotajlo said in a recent interview.</p><p>There’s no shortage of speculation about A.I. these days. San Francisco has been gripped by A.I. fervor, and the Bay Area’s tech scene has become a collection of warring tribes and splinter sects, each one convinced that it knows how the future will unfold.</p><p>Some A.I. predictions have taken the form of a manifesto, such as “<a href="https://darioamodei.com/machines-of-loving-grace" title="" rel="noopener noreferrer" target="_blank">Machines of Loving Grace</a>,” an 14,000-word essay written last year by Dario Amodei, the chief executive of Anthropic, or “<a href="https://situational-awareness.ai/" title="" rel="noopener noreferrer" target="_blank">Situational Awareness</a>,” a report by the former OpenAI researcher Leopold Aschenbrenner that was widely read in policy circles.</p><p>The people at the A.I. Futures Project designed theirs as a forecast scenario — essentially, a piece of rigorously researched science fiction that uses their best guesses about the future as plot points. The group spent nearly a year honing hundreds of predictions about A.I. Then, they brought in a writer — Scott Alexander, who writes the blog Astral Codex Ten — to help turn their forecast into a narrative.</p></div></div><div data-testid="companionColumn-2"><div><p>“We took what we thought would happen and tried to make it engaging,” Mr. Lifland said.</p><p>Critics of this approach might argue that fictional A.I. stories are better at spooking people than educating them. And some A.I. experts will no doubt object to the group’s central claim that artificial intelligence will overtake human intelligence.</p><p>Ali Farhadi, the chief executive of the Allen Institute for Artificial Intelligence, an A.I. lab in Seattle, reviewed the “AI 2027” report and said he wasn’t impressed.</p><p>“I’m all for projections and forecasts, but this forecast doesn’t seem to be grounded in scientific evidence, or the reality of how things are evolving in A.I.,” he said.</p><p>There’s no question that some of the group’s views are extreme. (Mr. Kokotajlo, for example, told me last year that he believed there was a <a href="https://www.nytimes.com/2024/06/04/technology/openai-culture-whistleblowers.html" title="">70 percent chance</a> that A.I. would destroy or catastrophically harm humanity.) And Mr. Kokotajlo and Mr. Lifland both have ties to Effective Altruism, another philosophical movement popular among tech workers<span>  </span>that has been making dire warnings about A.I. for years.</p><p>But it’s also worth noting that some of Silicon Valley’s largest companies are <a href="https://www.axios.com/2025/04/02/google-agi-deepmind-safety" title="" rel="noopener noreferrer" target="_blank">planning for a world</a> beyond A.G.I., and that many of the crazy-seeming predictions made about A.I. in the past — such as the view that machines would pass the Turing Test, a thought experiment that determines whether a machine can appear to communicate like a human — have <a href="https://arxiv.org/abs/2503.23674" title="" rel="noopener noreferrer" target="_blank">come true</a>.</p></div></div><div data-testid="companionColumn-3"><div><p>In 2021, the year before ChatGPT launched, Mr. Kokotajlo <a href="https://www.alignmentforum.org/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like" title="" rel="noopener noreferrer" target="_blank">wrote a blog post</a> titled “What 2026 Looks Like,” outlining his view of how A.I. systems would progress. A number of his predictions proved prescient, and he became convinced that this kind of forecasting was valuable, and that he was good at it.</p><p>“It’s an elegant, convenient way to communicate your view to other people,” he said.</p><p>Last week, Mr. Kokotajlo and Mr. Lifland invited me to their office — a small room in a Berkeley co-working space called Constellation, where a number of A.I. safety organizations hang a shingle — to show me how they operate.</p><p>Mr. Kokotajlo, wearing a tan military-style jacket, grabbed a marker and wrote four abbreviations on a large whiteboard: SC &gt; SAR &gt; SIAR &gt; ASI. Each one, he explained, represented a milestone in A.I. development.</p></div></div><div data-testid="ImageBlock-7"><div data-testid="imageblock-wrapper"><figure aria-label="media" role="group"><div data-testid="photoviewer-children-figure"><p><span>Image</span></p><picture><source media="(max-width: 599px) and (min-device-pixel-ratio: 3),(max-width: 599px) and (min-resolution: 3dppx),(max-width: 599px) and (min-resolution: 288dpi)" srcset="https://static01.nyt.com/images/2025/04/01/multimedia/00biz-roose-21-zlbv/00biz-roose-21-zlbv-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1800"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 2),(max-width: 599px) and (min-resolution: 2dppx),(max-width: 599px) and (min-resolution: 192dpi)" srcset="https://static01.nyt.com/images/2025/04/01/multimedia/00biz-roose-21-zlbv/00biz-roose-21-zlbv-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1200"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 1),(max-width: 599px) and (min-resolution: 1dppx),(max-width: 599px) and (min-resolution: 96dpi)" srcset="https://static01.nyt.com/images/2025/04/01/multimedia/00biz-roose-21-zlbv/00biz-roose-21-zlbv-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=600"/><img alt="Two men pose for a photo one either side of large computer screens." src="https://static01.nyt.com/images/2025/04/01/multimedia/00biz-roose-21-zlbv/00biz-roose-21-zlbv-articleLarge.jpg?quality=75&amp;auto=webp&amp;disable=upscale" srcset="https://static01.nyt.com/images/2025/04/01/multimedia/00biz-roose-21-zlbv/00biz-roose-21-zlbv-articleLarge.jpg?quality=75&amp;auto=webp 600w,https://static01.nyt.com/images/2025/04/01/multimedia/00biz-roose-21-zlbv/00biz-roose-21-zlbv-jumbo.jpg?quality=75&amp;auto=webp 820w,https://static01.nyt.com/images/2025/04/01/multimedia/00biz-roose-21-zlbv/00biz-roose-21-zlbv-superJumbo.jpg?quality=75&amp;auto=webp 1639w" sizes="((min-width: 600px) and (max-width: 1004px)) 84vw, (min-width: 1005px) 80vw, 100vw" uri="nyt://image/4d3340a1-f630-571b-bc63-2e0ddc6fa248" decoding="async" width="600" height="750"/></picture></div><figcaption data-testid="photoviewer-children-caption"><span>Researcher Eli Lifland and executive director Daniel Kokotajlo of the AI Futures Project, at their office space in Berkeley, Calif. </span><span><span>Credit...</span><span><span aria-hidden="false">Ian C. Bates for The New York Times</span></span></span></figcaption></figure></div></div><div data-testid="companionColumn-4"><p>First, he said, sometime in early 2027, if current trends hold, A.I. will be a superhuman coder. Then, by mid-2027, it will be a superhuman A.I. researcher — an autonomous agent that can oversee teams of A.I. coders and make new discoveries. Then, in late 2027 or early 2028, it will become a super<em>intelligent</em> A.I. researcher — a machine intelligence that knows more than we do about building advanced A.I., and can automate its own research and development, essentially building smarter versions of itself. From there, he said, it’s a short hop to artificial superintelligence, or A.S.I., at which point all bets are off.</p></div><div data-testid="companionColumn-5"><div><p>If all of this sounds fantastical … well, it is. Nothing remotely like what Mr. Kokotajlo and Mr. Lifland are predicting is possible with today’s A.I. tools, which can barely <a href="https://www.nytimes.com/2025/02/01/technology/openai-operator-agent.html" title="">order a burrito on DoorDash</a> without getting stuck.</p><p>But they are confident that these blind spots will shrink quickly, as A.I. systems become good enough at coding to accelerate A.I. research and development.</p><p>Their report focuses on OpenBrain, a fictional A.I. company that builds a powerful A.I. system known as Agent-1. (They decided against singling out a particular A.I. company, instead creating a composite out of the leading American A.I. labs.)</p><p>As Agent-1 gets better at coding, it begins to automate much of the engineering work at OpenBrain, which allows the company to move faster and helps build Agent-2, an even more capable A.I. researcher. By late 2027, when the scenario ends, Agent-4 is making a year’s worth of A.I. research breakthroughs every week, and threatens to go rogue.</p><p>I asked Mr. Kokotajlo what he thought would happen after that. Did he think, for example, that life in the year 2030 would still be recognizable? Would the streets of Berkeley be filled with humanoid robots? People texting their A.I. girlfriends? Would any of us have jobs?</p></div></div><div data-testid="companionColumn-6"><div><p>He gazed out the window, and admitted that he wasn’t sure. If the next few years went well and we kept A.I. under control, he said, he could envision a future where most people’s lives were still largely the same, but where nearby “special economic zones” filled with hyper-efficient robot factories would churn out everything we needed.</p><p>And if the next few years didn’t go well?</p><p>“Maybe the sky would be filled with pollution, and the people would be dead?” he said nonchalantly. “Something like that.”</p><p>One risk of dramatizing your A.I. predictions this way is that if you’re not careful, measured scenarios can veer into apocalyptic fantasies. Another is that, by trying to tell a dramatic story that captures people’s attention, you risk missing more boring outcomes, such as the scenario in which A.I. is generally well behaved and doesn’t cause much trouble for anyone.</p><p>Even though I agree with the authors of “AI 2027” that <a href="https://www.nytimes.com/2025/03/14/technology/why-im-feeling-the-agi.html" title="">powerful A.I. systems are coming soon</a>, I’m not convinced that superhuman A.I. coders will automatically pick up the other skills needed to bootstrap their way to general intelligence. And I’m wary of predictions that assume that A.I. progress will be smooth and exponential, with no major bottlenecks or roadblocks along the way.</p><p>But I think this kind of forecasting is worth doing, even if I disagree with some of the specific predictions. If powerful A.I. is really around the corner, we’re all going to need to start imagining some very strange futures.</p></div></div></section><div><div><div><div><p>Kevin Roose is a Times technology columnist and a host of the podcast &#34;<a href="https://www.nytimes.com/column/hard-fork">Hard Fork</a>.&#34;<span> </span></p></div></div></div></div><div><div><section id="styln-guide" role="complementary" aria-labelledby="styln-guide-title"><hr/><h2 id="styln-guide-title">Explore Our Coverage of Artificial Intelligence</h2><hr/><p><strong>News</strong><strong> and Analysis</strong></p><ul><li><p><strong>ChatGPT:</strong> OpenAI beefed up its chatbot with new technology <a href="https://www.nytimes.com/2025/03/25/technology/chatgpt-image-generator.html">designed to generate images</a> from detailed, complex and unusual instructions.</p></li><li><p><strong>CoreWeave: </strong>The company, which provides computing power for A.I., was founded by three Bitcoin enthusiasts. The company is now set to make the <a href="https://www.nytimes.com/2025/03/18/technology/coreweave-wall-street-ai-ipo.html">first prominent A.I. initial public offering</a>.</p></li><li><p><strong>DeepSeek: </strong>Since the founder of the Chinese artificial intelligence start-up shook hands with Xi Jinping, China’s leader, officials in China have been racing to <a href="https://www.nytimes.com/2025/03/18/business/china-government-deepseek.html">show how they are using the company’s technology</a>.</p></li><li><p><strong>Turing Award: </strong>The award, often called the Nobel Prize of computing, was given to Andrew Barto and Richard Sutton, the developers of a technique called reinforcement learning that is <a href="https://www.nytimes.com/2025/03/05/technology/turing-award-andrew-barto-richard-sutton.html">vital to chatbots</a>.</p></li></ul><hr/><p><strong>The Age of A.I.</strong></p><ul><li><p><strong>Exploring Mysteries of Delacroix:</strong> Eric and Wendy Schmidt and the Sorbonne will fund a new A.I. program to <a href="https://www.nytimes.com/2025/03/26/arts/design/digital-delacroix-ai-eric-wendy-schmidt-murals.html">digitize the master of Romanticism’s papers</a> and identify other artists who may have contributed to his murals and paintings.</p></li><li><p><strong>New Uses for Old Drugs: </strong>Scientists are using machine learning to <a href="https://www.nytimes.com/2025/03/20/well/ai-drug-repurposing.html">find new treatments</a> among thousands of old medicines.</p></li></ul></section></div></div></article></div></div></div>
  </body>
</html>
