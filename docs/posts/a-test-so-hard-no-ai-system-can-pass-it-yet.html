<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>A Test So Hard No AI System Can Pass It — Yet | Khanh's feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nytimes.com/2025/01/23/technology/ai-test-humanitys-last-exam.html">Original</a>
    <h1>A Test So Hard No AI System Can Pass It — Yet</h1>
    
    <div id="readability-page-1" class="page"><div id="site-content"><div><article id="story"><section name="articleBody"><div data-testid="companionColumn-0"><div><p>If you’re looking for a new reason to be nervous about artificial intelligence, try this: Some of the smartest humans in the world are struggling to create tests that A.I. systems can’t pass.</p><p>For years, A.I. systems were measured by giving new models a variety of standardized benchmark tests. Many of these tests consisted of challenging, S.A.T.-caliber problems in areas like math, science and logic. Comparing the models’ scores over time served as a rough measure of A.I. progress.</p><p>But A.I. systems eventually got too good at those tests, so new, harder tests were created — often with the types of questions graduate students might encounter on their exams.</p><p>Those tests aren’t in good shape, either. New models from companies like OpenAI, Google and Anthropic have been getting high scores on many Ph.D.-level challenges, limiting those tests’ usefulness and leading to a chilling question: Are A.I. systems getting too smart for us to measure?</p></div></div><div data-testid="companionColumn-1"><div><p>This week, researchers at the Center for AI Safety and Scale AI are releasing a possible answer to that question: A new evaluation, called “<a href="https://lastexam.ai/" title="" rel="noopener noreferrer" target="_blank">Humanity’s Last Exam</a>,” that they claim is the hardest test ever administered to A.I. systems. </p><p>Humanity’s Last Exam is the brainchild of Dan Hendrycks, a well-known A.I. safety researcher and director of the Center for AI Safety. (The test’s original name, “Humanity’s Last Stand,” was discarded for being overly dramatic.)</p><p>Mr. Hendrycks worked with Scale AI, an A.I. company where he is an advisor, to compile the test, which consists of roughly 3,000 multiple-choice and short answer questions designed to test A.I. systems’ abilities in areas ranging from analytic philosophy to rocket engineering.</p><p>Questions were submitted by experts in these fields, including college professors and prizewinning mathematicians, who were asked to come up with extremely difficult questions they knew the answers to. </p></div></div><div data-testid="companionColumn-2"><div><p>Here, try your hand at a question about hummingbird anatomy from the test:</p><blockquote><p>Hummingbirds within Apodiformes uniquely have a bilaterally paired oval bone, a sesamoid embedded in the caudolateral portion of the expanded, cruciate aponeurosis of insertion of m. depressor caudae. How many paired tendons are supported by this sesamoid bone? Answer with a number.</p></blockquote><p>Or, if physics is more your speed, try this one: </p><blockquote><p>A block is placed on a horizontal rail, along which it can slide frictionlessly. It is attached to the end of a rigid, massless rod of length R. A mass is attached at the other end. Both objects have weight W. The system is initially stationary, with the mass directly above the block. The mass is given an infinitesimal push, parallel to the rail. Assume the system is designed so that the rod can rotate through a full 360 degrees without interruption. When the rod is horizontal, it carries tension T1​. When the rod is vertical again, with the mass directly below the block, it carries tension T2. (Both these quantities could be negative, which would indicate that the rod is in compression.) What is the value of (T1−T2)/W?</p></blockquote><p>(I would print the answers here, but that would spoil the test for any A.I. systems being trained on this column. Also, I’m far too dumb to verify the answers myself.)</p></div></div><div data-testid="ImageBlock-5"><div data-testid="imageblock-wrapper"><figure aria-label="media" role="group"><div data-testid="photoviewer-children-figure"><p><span>Image</span></p><picture><source media="(max-width: 599px) and (min-device-pixel-ratio: 3),(max-width: 599px) and (min-resolution: 3dppx),(max-width: 599px) and (min-resolution: 288dpi)" srcset="https://static01.nyt.com/images/2025/01/22/multimedia/00roose-dan-fzgl/00roose-dan-fzgl-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1800"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 2),(max-width: 599px) and (min-resolution: 2dppx),(max-width: 599px) and (min-resolution: 192dpi)" srcset="https://static01.nyt.com/images/2025/01/22/multimedia/00roose-dan-fzgl/00roose-dan-fzgl-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1200"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 1),(max-width: 599px) and (min-resolution: 1dppx),(max-width: 599px) and (min-resolution: 96dpi)" srcset="https://static01.nyt.com/images/2025/01/22/multimedia/00roose-dan-fzgl/00roose-dan-fzgl-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=600"/><img alt="A seated man in a gray shirt poses for a photo." src="https://static01.nyt.com/images/2025/01/22/multimedia/00roose-dan-fzgl/00roose-dan-fzgl-articleLarge.jpg?quality=75&amp;auto=webp&amp;disable=upscale" srcset="https://static01.nyt.com/images/2025/01/22/multimedia/00roose-dan-fzgl/00roose-dan-fzgl-articleLarge.jpg?quality=75&amp;auto=webp 600w,https://static01.nyt.com/images/2025/01/22/multimedia/00roose-dan-fzgl/00roose-dan-fzgl-jumbo.jpg?quality=75&amp;auto=webp 1024w,https://static01.nyt.com/images/2025/01/22/multimedia/00roose-dan-fzgl/00roose-dan-fzgl-superJumbo.jpg?quality=75&amp;auto=webp 2048w" sizes="((min-width: 600px) and (max-width: 1004px)) 84vw, (min-width: 1005px) 80vw, 100vw" uri="nyt://image/5b5cbc69-ddc3-5e6b-991d-f3a99f95a55f" decoding="async" width="600" height="451"/></picture></div><figcaption data-testid="photoviewer-children-caption"><span>Humanity’s Last Exam is the brainchild of Dan Hendrycks, an A.I. safety researcher and director of the Center for AI Safety.</span><span><span>Credit...</span><span><span aria-hidden="false">Guerin Blask for The New York Times</span></span></span></figcaption></figure></div></div><div data-testid="companionColumn-3"><div><p>The questions on Humanity’s Last Exam went through a two-step filtering process. First, submitted questions were given to leading A.I. models to solve.</p><p>If the models couldn’t answer them (or if, in the case of multiple-choice questions, the models did worse than by random guessing), the questions were given to a set of human reviewers, who refined them and verified the correct answers. Experts who wrote top-rated questions were paid between $500 and $5,000 per question, as well as receiving credit for contributing to the exam.</p><p>Kevin Zhou, a postdoctoral researcher in theoretical particle physics at the University of California, Berkeley, submitted a handful of questions to the test. Three of his questions were chosen, all of which he told me were “along the upper range of what one might see in a graduate exam.” </p><p>Mr. Hendrycks, who helped create a widely used A.I. test known as Massive Multitask Language Understanding, or M.M.L.U., said he was inspired to create harder A.I. tests by a conversation with Elon Musk. (Mr. Hendrycks is also a safety advisor to Mr. Musk’s A.I. company, xAI.) Mr. Musk, he said, raised concerns about the existing tests given to A.I. models, which he thought were too easy.</p></div></div><div data-testid="companionColumn-4"><div><p>“Elon looked at the M.M.L.U. questions and said, ‘These are undergrad level. I want things that a world-class expert could do,’” Mr. Hendrycks said.</p><p>There are other tests trying to measure advanced A.I. capabilities in certain domains, such as FrontierMath, a test developed by Epoch AI, and <a href="https://arcprize.org/arc" title="" rel="noopener noreferrer" target="_blank">ARC-AGI</a>, a test<span>  </span>developed by the A.I. researcher François Chollet. </p><p>But Humanity’s Last Exam is aimed at determining how good A.I. systems are at answering complex questions across a wide variety of academic subjects, giving us what might be thought of as a general intelligence score.</p><p>“We are trying to estimate the extent to which A.I. can automate a lot of really difficult intellectual labor,” Mr. Hendrycks said.</p><p>Once the list of questions had been compiled, the researchers gave Humanity’s Last Exam to six leading A.I. models, including Google’s Gemini 1.5 Pro and Anthropic’s Claude 3.5 Sonnet. All of them failed miserably. OpenAI’s o1 system scored the highest of the bunch, with a score of 8.3 percent.</p></div></div><div data-testid="companionColumn-5"><div><p>(The New York Times has <a href="https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html" title="">sued</a> OpenAI and its partner, Microsoft, accusing them of copyright infringement of news content related to A.I. systems. OpenAI and Microsoft have denied those claims.)</p><p>Mr. Hendrycks said he expected those scores to rise quickly, and potentially to surpass 50 percent by the end of the year. At that point, he said, A.I. systems might be considered “world-class oracles,” capable of answering questions on any topic more accurately than human experts. And we might have to look for other ways to measure A.I.’s impacts, like looking at economic data or judging whether it can make novel discoveries in areas like math and science.</p><p>“You can imagine a better version of this where we can give questions that we don’t know the answers to yet, and we’re able to verify if the model is able to help solve it for us,” said Summer Yue, Scale AI’s director of research and an organizer of the exam.</p><p>Part of what’s so confusing about A.I. progress these days is how jagged it is. We have A.I. models capable of <a href="https://www.nytimes.com/2024/11/17/health/chatgpt-ai-doctors-diagnosis.html" title="">diagnosing diseases more effectively than human doctors</a>, <a href="https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/" title="" rel="noopener noreferrer" target="_blank">winning silver medals at the International Math Olympiad</a> and <a href="https://www.youtube.com/live/SKBG1sqdyIU?si=bq2zPAx1NrBxHPTf&amp;t=149" title="" rel="noopener noreferrer" target="_blank">beating top human programmers</a> on competitive coding challenges. </p><p>But these same models sometimes struggle with basic tasks, like arithmetic or writing metered poetry. That has given them a reputation as astoundingly brilliant at some things and totally useless at others, and it has created vastly different impressions of how fast A.I. is improving, depending on whether you’re looking at the best or the worst outputs. </p></div></div><div data-testid="companionColumn-6"><div><p>That jaggedness has also made measuring these models hard. I wrote last year that <a href="https://www.nytimes.com/2024/04/15/technology/ai-models-measurement.html" title="">we need better evaluations for A.I. systems</a>. I still believe that. But I also believe that we need more creative methods of tracking A.I. progress that don’t rely on standardized tests, because most of what humans do — and what we fear A.I. will do better than us — can’t be captured on a written exam.</p><p>Mr. Zhou, the theoretical particle physics researcher who submitted questions to Humanity’s Last Exam, told me that while A.I. models were often impressive at answering complex questions, he didn’t consider them a threat to him and his colleagues, because their jobs involve much more than spitting out correct answers.</p><p>“There’s a big gulf between what it means to take an exam and what it means to be a practicing physicist and researcher,” he said. “Even an A.I. that can answer these questions might not be ready to help in research, which is inherently less structured.”</p></div></div></section><div><div><div><div><p>Kevin Roose is a Times technology columnist and a host of the podcast &#34;<a href="https://www.nytimes.com/column/hard-fork">Hard Fork</a>.&#34;<span> </span></p></div></div></div></div><div><div><section id="styln-guide" role="complementary" aria-labelledby="styln-guide-title"><hr/><h2 id="styln-guide-title">Explore Our Coverage of Artificial Intelligence</h2><hr/><p><strong>News</strong><strong> and Analysis</strong></p><ul><li><p>President Trump announced a <a href="https://www.nytimes.com/2025/01/21/technology/trump-openai-stargate-artificial-intelligence.html">joint venture between OpenAI, SoftBank and Oracle</a> to create at least $100 billion in computing infrastructure to power A.I. But Elon Musk, a close adviser to Trump, <a href="https://www.nytimes.com/2025/01/22/us/politics/elon-musk-trump-stargate-ai-announcement.html">has cast doubt on the venture</a>.</p></li><li><p>Anthropic is said to be in talks to raise a new round of funding <a href="https://www.nytimes.com/2025/01/07/technology/anthropic-ai-funding.html">that could value the A.I. start-up at $60 billion</a>, following recent financing efforts <a href="https://www.nytimes.com/2024/12/24/technology/elon-musk-xai-funding.html">by xAI, owned by Musk</a>, and OpenAI, the market leader valued at $157 billion.</p></li><li><p>OpenAI revealed details about its plans to <a href="https://www.nytimes.com/2024/12/27/technology/openai-public-benefit-corporation.html">adopt a new corporate structure</a> that will remove the company from control by a nonprofit.</p></li></ul><hr/><p><strong>The Age of A.I.</strong></p><ul><li><p>Religious leaders are experimenting with A.I. in their work, spurring an industry of <a href="https://www.nytimes.com/2025/01/03/technology/ai-religious-leaders.html">faith-based tech companies that offer A.I. tools</a>, from assistants that can do theological research to chatbots that can help write sermons.</p></li><li><p>As A.I. is widely adopted, some <a href="https://www.nytimes.com/2024/12/26/technology/ai-economy-workers.html">once-struggling midsize U.S. cities</a> in the Midwest, Mid-Atlantic and South may benefit, new research predicts.</p></li><li><p>Coding boot camps once looked like the golden ticket to an economically secure future. <a href="https://www.nytimes.com/2024/11/24/business/computer-coding-boot-camps.html">But do they make sense in an A.I. world?</a></p></li></ul></section></div></div></article></div></div></div>
  </body>
</html>
