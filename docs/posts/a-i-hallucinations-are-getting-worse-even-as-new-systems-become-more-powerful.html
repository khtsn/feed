<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>A.I. Hallucinations Are Getting Worse, Even as New Systems Become More Powerful | Khanh's feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nytimes.com/2025/05/05/technology/ai-hallucinations-chatgpt-google.html">Original</a>
    <h1>A.I. Hallucinations Are Getting Worse, Even as New Systems Become More Powerful</h1>
    
    <div id="readability-page-1" class="page"><div id="site-content"><div><article id="story"><section name="articleBody"><div data-testid="companionColumn-0"><div><p>Last month, an A.I. bot that handles tech support for Cursor, <a href="https://www.cursor.com/" title="" rel="noopener noreferrer" target="_blank">an up-and-coming tool for computer programmers</a>, alerted several customers about a change in company policy. It said they were no longer allowed to use Cursor on more than just one computer.</p><p>In angry posts to <a href="https://news.ycombinator.com/item?id=43683012" title="" rel="noopener noreferrer" target="_blank">internet message boards</a>, the customers complained. Some canceled their Cursor accounts. And some got even angrier when they realized what had happened: The A.I. bot had announced a policy change that did not exist.</p><p>“We have no such policy. You’re of course free to use Cursor on multiple machines,” the company’s chief executive and co-founder, Michael Truell, <a href="https://old.reddit.com/r/cursor/comments/1jyy5am/psa_cursor_now_restricts_logins_to_a_single/" title="" rel="noopener noreferrer" target="_blank">wrote</a> in a Reddit post. “Unfortunately, this is an incorrect response from a front-line A.I. support bot.”</p><p>More than two years after <a href="https://www.nytimes.com/2022/12/10/technology/ai-chat-bot-chatgpt.html" title="">the arrival of ChatGPT</a>, tech companies, office workers and everyday consumers are using A.I. bots for an increasingly wide array of tasks. But there is still <a href="https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html" title="">no way of ensuring that these systems produce accurate information</a>.</p></div></div><div data-testid="companionColumn-1"><div><p>The newest and most powerful technologies — so-called <a href="https://www.nytimes.com/2025/03/26/technology/ai-reasoning-chatgpt-deepseek.html" title="">reasoning systems</a> from companies like OpenAI, Google and the Chinese start-up DeepSeek — are generating more errors, not fewer. As their math skills have notably improved, their handle on facts has gotten shakier. It is not entirely clear why.</p><p>Today’s A.I. bots are based on <a href="https://www.nytimes.com/2018/03/06/technology/google-artificial-intelligence.html" title="">complex mathematical systems</a> that learn their skills by analyzing enormous amounts of digital data. They do not — and cannot — decide what is true and what is false. Sometimes, they just make stuff up, a phenomenon some A.I. researchers call hallucinations. On one test, the hallucination rates of newer A.I. systems were as high as 79 percent.</p><p>These systems use mathematical probabilities to guess the best response, not a strict set of rules defined by human engineers. So they make a certain number of mistakes. “Despite our best efforts, they will always hallucinate,” said Amr Awadallah, the chief executive of Vectara, a start-up that builds A.I. tools for businesses, and a former Google executive. “That will never go away.”</p></div></div><div data-testid="ImageBlock-3"><div data-testid="imageblock-wrapper"><figure aria-label="media" role="group"><div data-testid="photoviewer-children-figure"><p><span>Image</span></p><picture><source media="(max-width: 599px) and (min-device-pixel-ratio: 3),(max-width: 599px) and (min-resolution: 3dppx),(max-width: 599px) and (min-resolution: 288dpi)" srcset="https://static01.nyt.com/images/2025/04/30/multimedia/00biz-hallucinations-Amr-1-mzvt/00biz-hallucinations-Amr-1-mzvt-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1800"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 2),(max-width: 599px) and (min-resolution: 2dppx),(max-width: 599px) and (min-resolution: 192dpi)" srcset="https://static01.nyt.com/images/2025/04/30/multimedia/00biz-hallucinations-Amr-1-mzvt/00biz-hallucinations-Amr-1-mzvt-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1200"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 1),(max-width: 599px) and (min-resolution: 1dppx),(max-width: 599px) and (min-resolution: 96dpi)" srcset="https://static01.nyt.com/images/2025/04/30/multimedia/00biz-hallucinations-Amr-1-mzvt/00biz-hallucinations-Amr-1-mzvt-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=600"/><img alt="Amr Awadallah, wearing a blue shirt, looks at a large computer monitor." src="https://static01.nyt.com/images/2025/04/30/multimedia/00biz-hallucinations-Amr-1-mzvt/00biz-hallucinations-Amr-1-mzvt-articleLarge.jpg?quality=75&amp;auto=webp&amp;disable=upscale" srcset="https://static01.nyt.com/images/2025/04/30/multimedia/00biz-hallucinations-Amr-1-mzvt/00biz-hallucinations-Amr-1-mzvt-articleLarge.jpg?quality=75&amp;auto=webp 600w,https://static01.nyt.com/images/2025/04/30/multimedia/00biz-hallucinations-Amr-1-mzvt/00biz-hallucinations-Amr-1-mzvt-jumbo.jpg?quality=75&amp;auto=webp 1024w,https://static01.nyt.com/images/2025/04/30/multimedia/00biz-hallucinations-Amr-1-mzvt/00biz-hallucinations-Amr-1-mzvt-superJumbo.jpg?quality=75&amp;auto=webp 2048w" sizes="((min-width: 600px) and (max-width: 1004px)) 84vw, (min-width: 1005px) 60vw, 100vw" uri="nyt://image/d6944100-db0c-5749-a8e7-78e742d24b54" decoding="async" width="600" height="450"/></picture></div><figcaption data-testid="photoviewer-children-caption"><span>Amr Awadallah, the chief executive of Vectara, which builds A.I. tools for businesses, believes A.I. “hallucinations” will persist.</span><span><span>Credit...</span><span><span aria-hidden="false">Cayce Clifford for The New York Times</span></span></span></figcaption></figure></div></div><div data-testid="companionColumn-2"><p>For several years, this phenomenon has raised concerns about the reliability of these systems. Though they are useful in some situations — like <a href="https://www.nytimes.com/2023/01/16/technology/chatgpt-artificial-intelligence-universities.html" title="">writing term papers</a>, summarizing office documents and <a href="https://www.nytimes.com/2021/09/09/technology/codex-artificial-intelligence-coding.html" title="">generating computer code</a> — their mistakes can cause problems.</p></div><div data-testid="companionColumn-3"><div><p>The A.I. bots tied to search engines like Google and Bing sometimes generate search results that are laughably wrong. If you ask them for a good marathon on the West Coast, they might suggest a race in Philadelphia. If they tell you the number of households in Illinois, they might cite a source that does not include that information.</p><p>Those hallucinations may not be a big problem for many people, but it is a serious issue for anyone using the technology with court documents, medical information or sensitive business data.</p><p>“You spend a lot of time trying to figure out which responses are factual and which aren’t,” said Pratik Verma, co-founder and chief executive of <a href="https://www.okahu.ai/" title="" rel="noopener noreferrer" target="_blank">Okahu</a>, a company that helps businesses navigate the hallucination problem. “Not dealing with these errors properly basically eliminates the value of A.I. systems, which are supposed to automate tasks for you.”</p><p>Cursor and Mr. Truell did not respond to requests for comment.</p><p>For more than two years, companies like OpenAI and Google steadily improved their A.I. systems and reduced the frequency of these errors. But with the use of new <a href="https://www.nytimes.com/2025/03/26/technology/ai-reasoning-chatgpt-deepseek.html" title="">reasoning systems</a>, errors are rising. The latest OpenAI systems hallucinate at a higher rate than the company’s previous system, according to the company’s own tests.</p><p>The company found that o3 — its most powerful system — hallucinated 33 percent of the time when running its PersonQA benchmark test, which involves answering questions about public figures. That is more than twice the hallucination rate of OpenAI’s previous reasoning system, called o1. The new o4-mini hallucinated at an even higher rate: 48 percent.</p></div></div><div data-testid="companionColumn-4"><p>When running another test called SimpleQA, which asks more general questions, the hallucination rates for o3 and o4-mini were 51 percent and 79 percent. The previous system, o1, hallucinated 44 percent of the time.</p></div><div data-testid="ImageBlock-9"><div data-testid="imageblock-wrapper"><figure aria-label="media" role="group"><div data-testid="photoviewer-children-figure"><p><span>Image</span></p><div data-testid="lazy-image"></div></div><figcaption data-testid="photoviewer-children-caption"><span>Since the arrival of ChatGPT, the phenomenon of hallucination has raised concerns about the reliability of A.I. systems.</span><span><span>Credit...</span><span><span aria-hidden="false">Kelsey McClellan for The New York Times</span></span></span></figcaption></figure></div></div><div data-testid="companionColumn-5"><div><p><a href="https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf" title="" rel="noopener noreferrer" target="_blank">In a paper detailing the tests</a>, OpenAI said more research was needed to understand the cause of these results. Because A.I. systems learn from more data than people can wrap their heads around, technologists struggle to determine why they behave in the ways they do.</p><p>“Hallucinations are not inherently more prevalent in reasoning models, though we are actively working to reduce the higher rates of hallucination we saw in o3 and o4-mini,” a company spokeswoman, Gaby Raila, said. “We’ll continue our research on hallucinations across all models to improve accuracy and reliability.”</p><p>Hannaneh Hajishirzi, a professor at the University of Washington and a researcher with the Allen Institute for Artificial Intelligence, is part of a team that recently devised a way of tracing a system’s behavior back to the <a href="https://allenai.org/blog/olmotrace" title="" rel="noopener noreferrer" target="_blank">individual pieces of data it was trained on</a>. But because systems learn from so much data — and because they can generate almost anything — this new tool can’t explain everything. “We still don’t know how these models work exactly,” she said.</p></div></div><div data-testid="companionColumn-6"><div><p>Tests by independent companies and researchers indicate that hallucination rates are also rising for reasoning models from companies such as Google and DeepSeek.</p><p>Since late 2023, Mr. Awadallah’s company, Vectara, has <a href="https://github.com/vectara/hallucination-leaderboard" title="" rel="noopener noreferrer" target="_blank">tracked how often chatbots veer from the truth</a>. The company asks these systems to perform a straightforward task that is readily verified: Summarize specific news articles. Even then, chatbots persistently invent information.</p><p>Vectara’s original research estimated that in this situation chatbots made up information at least 3 percent of the time and sometimes as much as 27 percent.</p><p>In the year and a half since, companies such as OpenAI and Google pushed those numbers down into the 1 or 2 percent range. Others, such as the San Francisco start-up Anthropic, hovered around 4 percent. But hallucination rates on this test have risen with reasoning systems. DeepSeek’s reasoning system, R1, hallucinated 14.3 percent of the time. OpenAI’s o3 climbed to 6.8.</p><p>(The New York Times has <a href="https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html" title="">sued</a> OpenAI and its partner, Microsoft, accusing them of copyright infringement regarding news content related to A.I. systems. OpenAI and Microsoft have denied those claims.)</p></div></div><div data-testid="companionColumn-7"><div><p>For years, companies like OpenAI relied on a simple concept: The more internet data they fed into their A.I. systems, <a href="https://www.nytimes.com/2024/04/06/technology/tech-giants-harvest-data-artificial-intelligence.html" title="">the better those systems would perform</a>. But they <a href="https://www.nytimes.com/2024/12/19/technology/artificial-intelligence-data-openai-google.html" title="">used up just about all the English text on the internet</a>, which meant they needed a new way of improving their chatbots.</p><p>So these companies are leaning more heavily on a technique that scientists call reinforcement learning. With this process, a system can learn behavior through trial and error. It is working well in certain areas, like math and computer programming. But it is falling short in other areas.</p><p>“The way these systems are trained, they will start focusing on one task — and start forgetting about others,” said Laura Perez-Beltrachini, a researcher at the University of Edinburgh who is among a <a href="https://arxiv.org/abs/2404.05904" title="" rel="noopener noreferrer" target="_blank">team closely examining the hallucination problem</a>.</p><p>Another issue is that reasoning models are designed to spend time “thinking” through complex problems before settling on an answer. As they try to tackle a problem step by step, they run the risk of hallucinating at each step. The errors can compound as they spend more time thinking.</p><p>The latest bots reveal each step to users, which means the users may see each error, too. Researchers have also found that in many cases, the steps displayed by a bot are <a href="https://www.anthropic.com/research/reasoning-models-dont-say-think" title="" rel="noopener noreferrer" target="_blank">unrelated to the answer it eventually delivers</a>.</p><p>“What the system says it is thinking is not necessarily what it is thinking,” said Aryo Pradipta Gema, an A.I. researcher at the University of Edinburgh and a fellow at Anthropic.</p></div></div></section><div><div><div><p>Cade Metz writes about artificial intelligence, driverless cars, robotics, virtual reality and other emerging areas of technology.</p></div><div><p>Karen Weise writes about technology for The Times and is based in Seattle. Her coverage focuses on Amazon and Microsoft, two of the most powerful companies in America.</p></div></div></div></article></div></div></div>
  </body>
</html>
