<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Why We’re Unlikely to Get Artificial General Intelligence Anytime Soon | Khanh's feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nytimes.com/2025/05/16/technology/what-is-agi.html">Original</a>
    <h1>Why We’re Unlikely to Get Artificial General Intelligence Anytime Soon</h1>
    
    <div id="readability-page-1" class="page"><div id="site-content"><div><article id="story"><section name="articleBody"><div data-testid="companionColumn-0"><div><p>Sam Altman, the chief executive of OpenAI, recently <a href="https://www.nytimes.com/2025/02/08/technology/sam-altman-elon-musk-trump.html" title="">told President Trump</a> during a private phone call that it would arrive before the end of his administration. Dario Amodei, the chief executive of Anthropic, OpenAI’s primary rival, repeatedly <a href="https://www.youtube.com/watch?v=Xywqm0vlUxk" title="" rel="noopener noreferrer" target="_blank">told podcasters</a> it could happen even sooner. The tech billionaire Elon Musk <a href="https://www.forbes.com/sites/ericsiegel/2024/04/10/artificial-general-intelligence-is-pure-hype/" title="" rel="noopener noreferrer" target="_blank">has said it could be here</a> before the end of the year.</p><p>Like many other voices across Silicon Valley and beyond, these executives predict that the arrival of artificial general intelligence, or A.G.I., is imminent.</p><p>Since the early 2000s, when a group of fringe researchers <a href="https://goertzel.org/who-coined-the-term-agi/" title="" rel="noopener noreferrer" target="_blank">slapped the term</a> on the cover of a book that described the autonomous computer systems they hoped to build one day, A.G.I. has served as shorthand for a future technology that achieves human-level intelligence. There is no settled definition of A.G.I., just an entrancing idea: an artificial intelligence that can match the many powers of the human mind.</p><p>Mr. Altman, Mr. Amodei and Mr. Musk have long chased this goal, as have executives and researchers at companies like <a href="https://www.nytimes.com/2023/12/03/technology/ai-openai-musk-page-altman.html" title="">Google</a> and <a href="https://www.nytimes.com/2023/05/16/technology/microsoft-ai-human-reasoning.html" title="">Microsoft</a>. And thanks, in part, to their fervent pursuit of this ambitious idea, they have produced technologies that are changing the way hundreds of millions of people research, make art and program computers. These technologies are now poised to transform entire professions.</p></div></div><div data-testid="companionColumn-1"><div><p>But since <a href="https://www.nytimes.com/2022/12/10/technology/ai-chat-bot-chatgpt.html" title="">the arrival of chatbots</a> like OpenAI’s ChatGPT, and <a href="https://www.nytimes.com/interactive/2025/03/26/business/ai-smarter-human-intelligence-puzzle.html" title="">the rapid improvement</a> of these strange and powerful systems over the last two years, many technologists have grown increasingly bold in predicting how soon A.G.I. will arrive. Some are even saying that once they deliver A.G.I., a more powerful creation called “<a href="https://ia.samaltman.com/" title="" rel="noopener noreferrer" target="_blank">superintelligence</a>” will follow.</p><p>As these eternally confident voices predict the near future, their speculations are getting ahead of reality. And though their companies are pushing the technology forward at a remarkable rate, an army of more sober voices are quick to dispel any claim that machines will soon match human intellect.</p><p>“The technology we’re building today is not sufficient to get there,” said Nick Frosst, a founder of the A.I. start-up Cohere who previously worked as a researcher at Google and <a href="https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html" title="">studied under</a> the most revered A.I. researcher of the last 50 years. “What we are building now are things that take in words and predict the next most likely word, or they take in pixels and predict the next most likely pixel. That’s very different from what you and I do.”</p><p>In a recent survey of the Association for the Advancement of Artificial Intelligence, a 40-year-old academic society that includes some of the most respected researchers in the field, more than three-quarters of respondents said the methods used to build today’s technology were <a href="https://www.nature.com/articles/d41586-025-00649-4" title="" rel="noopener noreferrer" target="_blank">unlikely to lead to A.G.I.</a></p><p>Opinions differ in part because scientists cannot even agree on a way of defining human intelligence, arguing endlessly over the merits and flaws of I.Q. tests and other benchmarks. Comparing our own brains to machines is even more subjective. This means that identifying A.G.I. is essentially a matter of opinion. (Last year, as part of a high-profile lawsuit, Mr. Musk’s attorneys <a href="https://www.nytimes.com/2024/03/02/technology/elon-musk-openai-lawsuit-microsoft-research.html" title="">said it was already here</a> because OpenAI, one of Mr. Musk’s chief rivals, has signed a contract with its main funder saying it will not sell products based on A.G.I. technology.)</p></div></div><div data-testid="companionColumn-2"><div><p>And scientists have no hard evidence that today’s technologies are capable of performing even some of the simpler things the brain can do, like recognizing irony or feeling empathy. Claims of A.G.I.’s imminent arrival are based on statistical extrapolations — and wishful thinking.</p><p>According to various benchmark tests, today’s technologies are improving at a consistent rate in some notable areas, like math and computer programming. But these tests describe only a small part of what people can do.</p><p>Humans know how to deal with a chaotic and constantly changing world. Machines struggle to master the unexpected — the challenges, both small and large, that do not look like what has happened in the past. Humans can dream up ideas that the world has never seen. Machines typically repeat or enhance what they have seen before.</p><p>That is why Mr. Frosst and other skeptics say pushing machines to human-level intelligence will require at least one big idea that the world’s technologists have not yet dreamed up. There is no way of knowing how long that will take.</p><p>“A system that’s better than humans in one way will not necessarily be better in other ways,” the Harvard cognitive scientist Steven Pinker said. “There’s just no such thing as an automatic, omniscient, omnipotent solver of every problem, including ones we haven’t even thought of yet. There’s a temptation to engage in a kind of magical thinking. But these systems are not miracles. They are very impressive gadgets.”</p></div></div><div data-testid="companionColumn-3"><div><h2 id="link-787d9095">‘A.I. Can Get There’</h2><p>Chatbots like ChatGPT are driven by what scientists call <a href="https://www.nytimes.com/2018/03/06/technology/google-artificial-intelligence.html" title="">neural networks</a>, mathematical systems that can identify patterns in text, images and sounds. By pinpointing patterns in vast troves of Wikipedia articles, news stories and chat logs, for instance, these systems can learn to <a href="https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html" title="">generate humanlike text</a> on their own, like poems and computer programs.</p><p>That means these systems are progressing much faster than computer technologies of the past. In previous decades, software engineers built applications one line of code at time, a tiny-step-by-tiny-step process that could never produce something as powerful as ChatGPT. Because neural networks can learn from data, they can reach new heights and reach them quickly.</p><p>After seeing the improvement of these systems over the last decade, some technologists believe the progress will continue at much the same rate — to A.G.I. and beyond.</p><p>“There are all these trends where all of the limitations are going away,” said Jared Kaplan, the chief science officer at Anthropic. “A.I. intelligence is quite different from human intelligence. Humans learn much more easily to do new tasks. They don’t need to practice as much as A.I. needs to. But eventually, with more practice, A.I. can get there.”</p><p>Among A.I. researchers, Dr. Kaplan is known for publishing a <a href="https://arxiv.org/pdf/2001.08361.pdf" title="" rel="noopener noreferrer" target="_blank">groundbreaking academic paper</a> that described what are now called “the Scaling Laws.” These laws essentially said: The more data an A.I. system analyzed, the better it would perform. Just as a student learns more by reading more books, an A.I. system finds more patterns in the text and learns to more accurately mimic the way people put words together.</p></div></div><div data-testid="companionColumn-4"><div><p>In recent months, companies like OpenAI and Anthropic used up <a href="https://www.nytimes.com/2024/12/19/technology/artificial-intelligence-data-openai-google.html" title="">just about all</a> of the English text on the internet, which meant they needed a new way of improving their chatbots. So they are leaning more heavily on a technique that scientists call reinforcement learning. Through this process, which can extend over weeks or months, a system can learn behavior through trial and error. By working through thousands of math problems, for instance, it can learn which techniques tend to lead to the right answer and which do not.</p><p>Thanks to this technique, researchers like Mr. Kaplan believe that the Scaling Laws (or something like them) will continue. As the technology continues to learn through trial and error across myriad fields, researchers say, it will follow the path of AlphaGo, a machine built in 2016 by a team of Google researchers.</p><p>Through reinforcement learning, AlphaGo learned to master the game of Go, a complex Chinese board game that is compared to chess, by playing millions of games against itself. That spring, it <a href="https://www.wired.com/2016/05/google-alpha-go-ai/" title="" rel="noopener noreferrer" target="_blank">beat one of the world’s best players</a>, stunning the A.I. community and the world. Most researchers had assumed that A.I. needed another 10 years to achieve such a feat.</p><p>AlphaGo played in ways no human ever had, teaching the top players new strategic approaches to this ancient game. For some, the belief is that systems like ChatGPT will take the same leap, reaching A.G.I. and then superintelligence.</p><p>But games like AlphaGo follow a small, limited set of rules. The real world is bounded only by the laws of physics. Modeling the entirety of the real world is well beyond today’s machines, so how can anyone be sure that A.G.I. — let alone superintelligence — is just around the corner?</p></div></div><div data-testid="companionColumn-5"><div><h2 id="link-2a37ea12">The Gap Between Humans and Machines</h2><p>It is indisputable that today’s machines have already eclipsed the human brain in some ways, but that has been true for a long time. A calculator can do basic math faster than a human. Chatbots like ChatGPT can write faster, and as they write, they can instantly draw on more texts than any human brain could ever read or remember. These systems are <a href="https://www.nytimes.com/2024/12/20/technology/openai-new-ai-math-science.html" title="">exceeding human performance</a> on some tests involving high-level math and coding.</p><p>But people cannot be reduced to these benchmarks. “There are many kinds of intelligence out there in the natural world,” said Josh Tenenbaum, a professor of computational cognitive science at the Massachusetts Institute of Technology.</p><p>One obvious difference is that human intelligence is tied to the physical world. It extends beyond words and numbers and sounds and images into the realm of tables and chairs and stoves and frying pans and buildings and cars and whatever else we encounter with each passing day. Part of intelligence is knowing when to flip a pancake sitting on the griddle.</p><p>Some companies are already <a href="https://www.nytimes.com/2025/04/04/technology/humanoid-robots-1x.html" title="">training humanoid robots</a> in much the same way that others are training chatbots. But this is more difficult and more time consuming than building ChatGPT, requiring extensive training in physical labs, warehouses and homes. Robotic research is years behind chatbot research.</p><p>The gap between human and machine is even wider. In both the physical and the digital realms, machines still struggle to match the parts of human intelligence that are harder to define.</p></div></div><div data-testid="companionColumn-6"><div><p>The new way of building chatbots, reinforcement learning, is working well in areas like math and computer programming, where companies can clearly define the good behavior and the bad. Math problems have undeniable answers. Computer programs must compile and run. But the technique doesn’t work as well with creative writing, philosophy or ethics.</p><p>Mr. Altman <a href="https://x.com/sama/status/1899535387435086115" title="" rel="noopener noreferrer" target="_blank">recently wrote on X</a> that OpenAI had trained a new system that was “good at creative writing.” It was the first time, he added, that “I have been really struck by something written by A.I.” Writing is what these systems do best. But “creative writing” is hard to measure. It takes different forms in different situations and exhibits characteristics that are not easy to explain, much less quantify: sincerity, humor, honesty.</p><p>As these systems are deployed into the world, humans tell them what to do and <a href="https://www.nytimes.com/interactive/2024/09/03/technology/zoox-self-driving-cars-remote-control.html" title="">guide them through</a> moments of novelty, change and uncertainty.</p><p>“A.I. needs us: living beings, producing constantly, feeding the machine,” said Matteo Pasquinelli, a professor of the philosophy of science at Ca’ Foscari University in Venice. “It needs the originality of our ideas and our lives.”</p><h2 id="link-5e2f8651">A Thrilling Fantasy</h2><p>For people both inside the tech industry and out, claims of imminent A.G.I. can be thrilling. Humans have dreamed of creating an artificial intelligence going back to the myth of the Golem, which appeared as early as the 12th century. This is the fantasy that drives works like Mary Shelley’s “Frankenstein” and Stanley Kubrick’s “2001: A Space Odyssey.”</p></div></div><div data-testid="companionColumn-7"><div><p>Now that many of us are using computer systems that can write and even talk like we do, <a href="https://www.nytimes.com/2023/01/20/technology/chatbots-turing-test.html" title="">it is only natural</a> for us to assume that intelligent machines are almost here. It is what we have anticipated for centuries.</p><p>When a group of academics founded the A.I. field in the late 1950s, they were sure it wouldn’t take very long to build computers that recreated the brain. Some argued that a machine would beat the world chess champion and discover its own mathematical theorem within a decade. But none of that happened on that time frame. Some of it still hasn’t.</p><p>Many of the people building today’s technology see themselves as fulfilling a kind of technological destiny, pushing toward an inevitable scientific moment, like the creation of fire or the atomic bomb. But they cannot point to a scientific reason that it will happen soon.</p><p>That is why many other scientists say no one will reach A.G.I. without a new idea — something beyond the powerful neural networks that merely find patterns in data. That new idea could arrive tomorrow. But even then, the industry would need years to develop it.</p><p>Yann LeCun, the chief A.I. scientist at Meta, has dreamed of building what we now call A.G.I. since he saw “2001: A Space Odyssey” in 70-millimeter Cinerama at a Paris movie theater when he was 9 years old. And he was among <a href="https://www.nytimes.com/2019/03/27/technology/turing-award-ai.html" title="">the three pioneers who won the 2018 Turing Award</a> — considered the Nobel Prize of computing — for their early work on neural networks. But he does not believe that A.G.I. is near.</p><p>At Meta, his research lab is looking beyond the neural networks that have entranced the tech industry. Mr. LeCun and his colleagues are searching for the missing idea. “A lot is riding on figuring out whether the next generation architecture will deliver human-level A.I. within the next 10 years,” he said. “It may not. At this point, we can’t tell.”</p></div></div></section><div><div><div><p>Cade Metz is a Times reporter who writes about artificial intelligence, driverless cars, robotics, virtual reality and other emerging areas of technology.</p></div></div></div><div><div><section id="styln-guide" role="complementary" aria-labelledby="styln-guide-title"><hr/><h2 id="styln-guide-title">Explore Our Coverage of Artificial Intelligence</h2><hr/><p><strong>News</strong><strong> and Analysis</strong></p><ul><li><p><strong>OpenAI in Talks to Acquire Windsurf:</strong> The potential $3 billion deal for the programming tool would be <a href="https://www.nytimes.com/2025/05/13/technology/openai-windsurf-talks.html">OpenAI&#39;s largest acquisition</a>, and is part of an effort to expand the company&#39;s offerings beyond its popular chatbot.</p></li><li><p><strong>Trump’s Middle East Visit: </strong>As President Trump heads to the region, America’s dominance over A.I. chips has become a <a href="https://www.nytimes.com/2025/05/12/business/economy/trump-middle-east-trip-semiconductor-deals.html">powerful source of leverage for the president</a>.</p></li><li><p><strong>Google’s A.I. Chatbot: </strong>The tech giant said it would make its Gemini chatbot <a href="https://www.nytimes.com/2025/05/02/technology/google-gemini-ai-chatbot-kids.html">available to children</a>, and warned families in an email about the changes.</p></li><li><p><strong>Venture Capital:</strong> Thrive Capital has bet big on artificial intelligence, including emerging giants of the field like OpenAI and Databricks. Now the venture capital firm is taking a different approach: <a href="https://www.nytimes.com/2025/04/29/business/dealbook/thrive-holdings-rollup-ai.html">creating and buying companies</a> that it believes can benefit from A.I.</p></li></ul><hr/><p><strong>The Age of A.I.</strong></p><ul><li><p><strong>Professors Using ChatGPT:</strong> Students call it hypocritical. A senior at Northeastern University demanded her tuition back. But instructors say generative A.I. tools <a href="https://www.nytimes.com/2025/05/14/technology/chatgpt-college-professors.html">make them better at their jobs</a>.</p></li><li><p><strong>An Agatha Christie Avatar: </strong>The long-dead British novelist has been reanimated and is “teaching” an online writing course. <a href="https://www.nytimes.com/2025/05/08/world/europe/agatha-christie-ai-class-bbc.html">But do we want to learn from a digital prosthetic</a> built by artificial intelligence?</p></li><li><p><strong>Writing Tools on Your Phone: </strong>Artificial intelligence software — some already free on your device — can quickly compose and edit documents. <a href="https://www.nytimes.com/2025/05/07/technology/personaltech/ai-writing-tools-phone.html">But be sure to check its work</a>.</p></li></ul></section></div></div></article></div></div></div>
  </body>
</html>
